---
title: "Methods"
date: "`r lubridate::today()`"
bibliography: references.bib
csl: apa.csl
output:
  pdf_document:
    includes:
      in_header: header.tex
  github_document: default
---

## Methods

### General description

We built an R [@rstat] based framework to simplify two aspects of systematic reviews:
record acquisition and classification.
The framework comprises several modules that communicate through intermediate outputs stored in standard formats, which make it possible for users to extend the framework or easily integrate it with other tools in their pipeline.
See Supplemental Material S1 for an in-depth description of the framework and how to use it.\
The tasks carried out by the framework are grouped into "sessions", i.e., a set of actions that starts from a search query to obtain a set of scientific citation data (records), which are then labelled as relevant ("positive" in the rest of the text) or not ("negative") for the topic of interest (Fig. 1).
The framework can generate a new query from this labelled set and perform a new session to find records possibly missed by the first query.\
The researcher initiates the process with a starting query derived by domain knowledge from which she expects a high relevant/non-relevant record ratio.\
Follows a description of the framework's components.\

```{r method_diagram, fig.cap = "Figure 1.Framework's visual depiction.", echo = FALSE, out.width = "100%"}
knitr::include_graphics("methods_diagram.png", auto_pdf = T)
```

<!-- ![Figure 1. Framework visual depiction.](methods_diagram.png) -->

### Record's acquisition and initial labelling
We built a set of tools to let users automatically search and download citation data from three major scientific databases ("sources"):
MEDLINE (<https://pubmed.ncbi.nlm.nih.gov/>), Web Of Science (WOS, <https://apps.webofknowledge.com/>) and the Institute of Electrical and Electronics Engineers (IEEE, <https://ieeexplore.ieee.org/Xplore/home.jsp>).
The framework takes care of authorization management for non-open databases like WOS and IEEE.
It is also possible to download and import records in the framework manually;
this is particularly useful to acquire records from the SCOPUS (<https://www.scopus.com/search/form.uri?display=basic#basic>) and EMBASE databases (<https://www.embase.com/#advancedSearch/default>), for which a comprehensive API interface was not easy to build.
A short guide on how to set up the framework for each supported database is available in Supplemental Material S3.\
The acquired records are merged into a single database, resolving duplicates and different formatting between sources. The records are ordered according to the frequency of the positive query terms (e.g., not preceded by a $NOT$ modifier) in the title and abstract ("simple query ordering").\
The researcher is then asked to label a number of records to create the "initial training set" needed to start the automatic classification.
We suggest manually labelling the first 250 records (see "hyperparameter optimization" later).
The simple query ordering increases the positivity rate in the initial training set [@wallace2010active], which provide higher sensitivity during automatic classification [@chawla2004special].

### Text feature extraction
The collected citation data have a number of fields characterizing a scientific publication.
The framework models the relevance of a record based on the following fields:
title, abstract, authors, keywords, MESH terms [@lipscomb2000medical].
A series of Natural Language Processing (NLP) techniques [@baeza1999modern; @marshall2015systematic; @ananiadou2006text] are employed to transform the textual information in these fields into features for machine learning through a bag-of-words approach [@marshall2015systematic].
The processing of free text fields (title, abstract) includes: tokenization (i.e., extracting the terms), common stopwords (i.e. sentence components bringing no meaning) removal, part-of-speech filtering (only nouns, adjectives, verbs and untagged terms are retained), and lemmatization of the terms (i.e. reduction to their base grammar form).
Text processing for authors, keywords and MESH terms identifies logical units (e.g., author's full names, composite keywords) and extracts them.\
Terms appearing in less than 5% of the labelled documents are removed from negative records.
All terms in the positive set are kept to increase sensitivity at the cost of specificity.\
Some terms tend to co-appear in records (non-consecutive ngrams, nc-ngrams), often carrying a particular meaning when copresent.
To detect nc-ngrams, we generated a word network representation [@rousseau2015graph] posing edges between terms with a cosine similarity in terms of record co-occurrence > 0.5.
We extracted the maximal cliques in the network [@eppstein2010listing] representing highly correlated groups of terms;
These groups of terms are added to the data set as individual features.
Only nc-ngrams comprising a maximum of ten terms are kept.\
A second network is built using a co-occurrence threshold of 0.9.
In this case, the cliques represent terms that always appear together and therefore can be considered redundant (i.e. they do not need to be considered separately).
These terms are merged to increase computation efficiency and reduce overfitting.\
The output is a Document-Term Matrix (DTM), with $N_d$ rows representing the records ($D_i$), $N_t$ terms column for the $t_{field}$ terms (divided by record field) and ${0,1}$ values whether $t_{field} \in D_i$.
We also enriched the DTM with features referencing the number of terms in each field to help the model scale term importance based on the field length.

### Label prediction
We used a Bayesian Additive Regression Trees (BART) machine learning "classification model" [@chipman2010bart] [in the implementation of @kapelner2013bartmachine] to predict the probability of a record of being relevant, given the information coded into the enriched DTM and the initial training set.
We set up the BART model to use 2000 MCMC iterations (after 250 burn-in iterations) and 50 trees;
we used a $k$ value of 2 to regularize extreme prediction and let the model use missing fields in the DTM as features [@kapelner2015prediction].
Positive records are oversampled ten times to increase sensitivity [@batista2004study].\
The output is a posterior predictive distribution (PPD) for each record describing its probability of being relevant (i.e., a positive match).
An ensemble of ten models was fitted to improve prediction stability by averaging the PPD between models [@zhou2021ensemble; @dietterich2000ensemble].\

To assign the labels, we employed an "active learning" [@settles2009active, @miwa2014reducing] approach, where a human reviews a specific subset of predictions made from the machine, which is then retrained on the manually reviewed dataset.
This process proceeds iteratively, decreasing prediction uncertainty.\
Label assignment is done through the identification of an "uncertainty zone" whose construction is possible thanks to the Bayesian nature of BART, which provides full PPDs instead of point-wise predictions for each record.\
To describe the process formally, we define

$$\pi_i = \frac{1}{M}\sum_{j=1}^MPr(L_i = \text{1}|DTM,m_j)$$

as the PPD of a record $D_i$ being relevant (i.e, having a positive label, $L_i = 1$), averaging the PPDs of the ensemble of $M=10$ models $m$, and

$$
\begin{aligned}
\pi_{i,l} = \{\pi_i : Pr(\pi_i) = 1\%\}\\
\pi_{i,u} = \{\pi_i : Pr(\pi_i) = 99\%\}
\end{aligned}
$$

as respectively the lower and upper boundaries of the 98% quantile interval of $\pi_i$ (98% Predictive Interval, 98% PrI).\
Then we identify the "uncertainty zone" as

$$U_\pi=[\max\vec{\pi}_{u}^-, \min\vec{\pi}_{l}^+]$$
with $\vec{\pi}_{u}^-$ being the vector of $\pi_{i,u}$ with a negative label and $\vec{\pi}_{l}^+$ the vector of $\pi_{i,l}$ with a positive label.
That is, $U_\pi$ defines a range of values between the smallest $\pi_{i,l}$ in the set of already labelled positive records $L_p$ and the largest $pi_{i,u}$ related to the negative ones $L_n$, noting that the two limits can appear in any order.\
Consequently, a record $D_i$ will be labelled as positive if

$$\pi_{i,l} > \max_{\pi \in U_\pi} \pi$$

that is, the record lower 98% PrI boundary should be higher than every value in the uncertainty zone.
In other words, for a record to be labelled positive, its PPD should be within the range of the mixture of PPD of the previously labelled positive records and not cross the distributions of the negative records.\
Conversely, a record is labelled as negative if

$$\pi_{i,u} < \min_{\pi \in U_\pi} \pi$$

All other records are labelled as "uncertain".

Manual review is then necessary for: 1) uncertain records, 2) positive records (to avoid false positives), 3) records whose predicted label differs from the existing manual one.
The last case helps identify human errors or inconsistent labelling criteria.

The automatic classification and the manual review steps alternate in a loop (CR iterations) until no new positive matches are found in four consecutive iterations.

### New search query generation
We created an algorithm that generates a new search query to acquire further relevant publications missed during the first search, possibly at a reasonable cost in specificity (i.e., a higher number of negative results).\
The algorithm encompasses a number of steps:

* We fit a partition tree [@rpart] between the DTM and 800 samples from the PPD;
if a term is present multiple times in the DTM (e.g. both title and abstract), they are counted just one, and field term count features are removed.
This step generates a list of rules composed by $AND$/$NOT$ "conditions" made of terms/authors/keywords/MESH tokens, which together identify a group of records.
* For each rule, negative conditions (i.e., $NOT$ statements) are added iteratively, starting from the most specific one, until no conditions are found that would not also remove positive records.
* The extended set of rules is sorted by positive-negative record difference in descending order.
The cumulative number of unique positive records is computed and used to group the rules.
Rules inside each group are ordered by specificity.
* The researcher is then asked to review the rule groups, selecting one or more rules (useful if they convey different meaning) from each, or edit them (in case too specific positive or negative conditions were included).
It is possible to exclude a group of rules altogether, especially those with the worse sensitivity/specificity ratio.
* The selected rules are joined together by $OR$ statements, defining a subset of records with a sensibly higher proportion of positive records than the original one.
* Redundant rules (i.e., rules whose positive records are already included in more specific ones) and conditions (i.e., conditions that once removed do not decrease the total number of positive or do not increase the negative records) are removed.
* Finally, the rules are re-elaborated in a query usable on the major scientific databases.

Since the algorithm is data-driven, it creates queries that effectively select positive records from the input dataset but may be not specific enough once applied to actual research databases.
Therefore we appended an extra subquery in $AND$, which specifies the general topics of our search and delimitates the search domain.\
The new query was used to initiate a second search session.

### Performance evaluation
We trained a simple Bayesian logistic regression (surrogate model) on the reviewed records to evaluate the classification model consistency (see Discussion for the theoretical justification).
The surrogate model uses as predictor the lower bound of the 98% PrI of the records' PPD with weakly regularizing, robust priors for the intercept (Student T with $\nu=3,\mu=0,\sigma=2.5$) and the linear coefficient (Student T with $\nu=3,\mu=0,\sigma=1.5$).\
The quality of the model was evaluated through Bayesian $R^2$ [@gelman2019r], of which we reported the posterior median and 90% Credible Interval [90% CrI].
The $R^2$ also provides an evaluation of the consistency of the original classification model.
Given that this model is conditional only on the BART predictions and not on the DTM, it is characterized by more uncertainty, providing plausible worst-case scenarios.\
The surrogate model is then used to generate the predictive cumulative distribution of the number of missed positive records in the whole dataset.
This distribution allows estimating the expected total posterior "Sensitivity" and the "Work saved over random" (WSoR) of the classification model in the full (unreviewed) dataset.
The WSoR is one minus the number of records to manually label at random to find the same number of positives; this last quantity is estimated through a negative hypergeometric distribution [@chae1993presenting].\
For the number of predicted positive records, the sensitivity and the WSoR, we reported the "truncated 90% PrI" [trunc. 90% PrI], which is the uncertainty interval bounded at the number of observed total positive records (i.e., there cannot be less predicted positive records than observed).

### Hyperparameter evaluation
Our classification algorithm has a number of hyperparameters:

* The number of ensemble models;
* The source of randomness between models in the ensemble, that is, either MCMC sampling only [@robert2004monte], or MCMC plus data bootstrapping [@breiman1996bagging] before training;
* The oversampling rate of positive records;
* The PrI quantiles to define the uncertainty zone;
* The size of the initial training set.

To evaluate the hyperparameter effect of performance, we set up a "grid search" [@claesen2015hyperparameter; @yang2020hyperparameter] on a prelabelled subset made of the first 1200 records from the first session dataset. See Supplemental Material S1 for the combinations of hyperparameter used and other details on the method.\
The framework tested each hyperparameter combination until four CR iterations with no positive records were returned or the whole dataset got labelled.\
For each combination, a performance score was computed as the product of *Efficiency* (1 minus the ratio of records that required review over the total) and *Sensitivity* (number of positive records found over the total of positives).
We then identified homogeneous "performance clusters" of hyperparameter values and scores using a partition tree.
For the rest of our study, we chose the best cluster of parameter value combinations and, inside the cluster, the best combination according to Sensitivity and Efficiency in this order.


