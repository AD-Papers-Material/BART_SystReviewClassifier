---
title: "Manuscript"
date: "`r lubridate::today()`"
author:
  - first_name: "Angelo"
    last_name: "D'Ambrosio"
    url: http://github.com/bakaburg1/
    affiliation: Institute for Infection Prevention and Hospital Hygiene, Freiburg University Hospital
    affiliation_url: https://www.uniklinik-freiburg.de/iuk.html
    orcid_id: 0000-0002-2045-5155
  - first_name: "Tjibbe"
    last_name: "Donker"
    url: http://github.com/bakaburg1/
    affiliation: Institute for Infection Prevention and Hospital Hygiene, Freiburg University Hospital
    affiliation_url: https://www.uniklinik-freiburg.de/iuk.html
    orcid_id: 0000-0002-2045-5155
output: distill::distill_article
---

```{r Setup, include=FALSE}

knitr::opts_knit$set(root.dir = ifelse(basename(getwd()) == 'Manuscript', dirname(getwd()), getwd()))

options(knitr.kable.NA = '')

knitr::opts_chunk$set(
	echo = FALSE,
	error = FALSE,
	message = FALSE,
	warning = FALSE,
	dpi = 400,
	cache = FALSE
)

# The following are already the default, but better be explicit
options(basren.probs = c(.05, .5, .95))
options(basren.sessions_folder = 'Sessions')

```

## Results

```{r Results}

source(file.path('R', 'Setup.R'))

session1_files <- get_session_files('Session1')
session2_files <- get_session_files('Session2')

search_results <- summarise_sources_by_session(keep_session_label = TRUE)

source_list <- source_session_summary_to_list(search_results)

search_results$Session_label <- NULL

annotation_results <- summarise_annotations_by_session(remove_raw_data = TRUE)

```

### First session

```{r session 1}
records_s1 <- import_data(session1_files$Records)

source_summary <- source_list$Session1

all_records <- source_summary[names(source_summary) != 'Total'] %>%
	sapply('[[', 'Records') %>% sum()

source_distr <- get_source_distribution(records_s1, format_fun = identity)

session_summary <- summarise_annotations('Session1', remove_raw_data = FALSE)

num_first_labeling <- session_summary[1,]$tot_reviewed_

num_first_positives <- session_summary[1,]$Positives
num_first_negatives <- session_summary[1,]$Negatives

num_last_positives <- session_summary$Positives %>% last()
num_last_negatives <- session_summary$Negatives %>% last()
tot_reviewed <- session_summary$tot_reviewed_ %>% last()

n_review_rounds <- nrow(session_summary) - 1

var_importance <- extract_var_imp('Session1') %>%
	format_var_imp(as_data_frame = FALSE) %>%
	paste(collapse = ', ')
	
```


The first search session returned a total of `r all_records` unique records, specifically `r source_summary$Embase$Records` (`r source_summary$Embase$Perc_over_total` of the total) records from the *EMBASE* database, followed by `r source_summary$Pubmed$Records` (`r source_summary$Pubmed$Perc_over_total`) from *Pubmed/Medline*, `r source_summary$Scopus$Records` (`r source_summary$Scopus$Perc_over_total`) from *Scopus*, `r source_summary$WOS$Records` (`r source_summary$WOS$Perc_over_total`) from *Web of Science*, and `r source_summary$IEEE$Records` (`r source_summary$IEEE$Perc_over_total`) from *IEEE* (Table 1). There were various degrees of overlapping with the `r percent(1 - source_distr[1])` of records being present in more than one database and *EMBASE* and *IEEE* being the databases with the higher uniqueness ratios. The final dataset was composed by `r source_summary$Total$Records` unique records.\
The first `r num_first_labeling` records were manually labeled. Of these `r sprintf('%d (%s)', num_first_positives, percent(num_first_positives/num_first_labeling))` were labeled as positive, and `r sprintf('%d (%s)', num_first_negatives, percent(num_first_negatives/num_first_labeling))` as negative.

`r knitr::kable(search_results, caption = 'Table 1.')`

After the first manual classification, `r n_review_rounds` classification and review rounds (CR) were performed (Table 2). It is possible to observe how the number of records that required manual review dropped fast between iterations, indicating that the engine was converging while the uncertainties were resolved.\
This phenomenon is better depicted in figure 1 in the Suppl. Mat. S2. It shows the mixture distribution of the predicted probabilities for each record, specifically for the reviewed positive and negative records, and for records that need manual review after the classification step: it can be noticed how the distribution of the uncertain records shrinks (they concentrate in a shorter probability range) and shifts toward the negative zone as more positive matches are found and reviewed. Accordingly (Table 2), the number of new positives found drops after the first couple of iterations.
As per default, the classification process was stopped after four rounds with no new positive matches; a total number of `r num_last_positives` positives were found over `r tot_reviewed` manually reviewed records (`r percent(num_last_positives/tot_reviewed)` positivity rate).\

`r knitr::kable(annotation_results, caption = 'Table 2.')`

By evaluating how often the BART algorithm decided to use a term for classification, it is possible to list a subset of the variables most relevant for the classification of the documents: `r var_importance`. In parenthesis, it is reported the part of the record in which the term was used, while the numeric values indicate respectively the rate of posterior trees (over 10000 trees) in which a term was used, and the statistical score of the regression model that assesses the linear direction of the association between a term and the probability of a positive match (see methods). The "&" indicates that two terms are present together in a document but not close to each other.\
The engine was able to pick up the central concept of the research (i.e., "patient transport" or "transfer" through a "network" of "facility"ies that facilitates the "spread" of infections) and even one of the authors of the current study (Donker T.) or the region of interest ("Orange County") of another research group active on the topic. It is interesting to see that some terms were considered highly relevant (e.g., "Worker" in 6th position) although in a simpler linear model, their effect would be hardly significant (statistic: -1.21), and this highlight the extra predictive power brought by a highly non-linear model.\
A more extensive set of terms is presented in Table 1 of Supplemental Material S2.

### Second session

```{r session 2}
new_query <- import_data('Session_journal.csv') %>% 
    filter(Session_ID == 'Session2') %>% 
    with(Query[1])

records_s2 <- import_data(session2_files$Records)

source_summary <- source_list$Session2

all_records <- source_summary[names(source_summary) != 'Total'] %>%
	sapply('[[', 'Records') %>% sum()

session_diff <- (
	source_list$Session1$Total$Records + source_list$Session2$Total$Records
	) - source_list$`All Sessions`$Total$Records

source_distr <- get_source_distribution(records_s2, format_fun = identity)

session_summary <- summarise_annotations('Session2', remove_raw_data = FALSE)

num_last_positives <- session_summary$Positives %>% last()

tot_reviewed <- session_summary$tot_reviewed_ %>% last()


n_review_rounds <- nrow(session_summary) - 1
```


The results of the first classification session were used to create a second, data-driven query with the purpose of performing a more large-spectrum search to find records which may have escape the first search session. The resulting query was the following:\

*`r new_query`*

The final piece *AND ((antimicrobial resistance) OR (healthcare infection)* was added manually to better define the search domain, since the algorithm was trained on documents that were all more or less related to these topics.\
The generated query also provides a more nuanced understanding of the engine's internal classification logic, and this is helpful to spot possible biases in the model.

The new search produced `r all_records` records (Table 1), of which `r source_summary$Embase$Records` (`r source_summary$Embase$Perc_over_total`) from the *EMBASE*, followed by `r source_summary$Pubmed$Records` (`r source_summary$Pubmed$Perc_over_total`) from *Pubmed/Medline*, `r source_summary$Scopus$Records` (`r source_summary$Scopus$Perc_over_total`) from *Scopus*, `r source_summary$WOS$Records` (`r source_summary$WOS$Perc_over_total`) from *Web of Science*, and `r source_summary$IEEE$Records` (`r source_summary$IEEE$Perc_over_total`) from *IEEE*; compared to the first session, the relative weight of *EMBASE* and *Pubmed* over the total was decreased, while the amount of content specificity was greatly increased, as it was for *Scopus*. After removal of duplicates, `r source_summary$Total$Records` unique records were obtained. Once joined with the session 1 records and duplicates remove, we obtained `r nrow(records_s2)` unique records, with just `r session_diff` shared records between searches, that is the `r percent(session_diff / source_list[['All Sessions']]$Total$Records)`. The percentage of records shared by two or more source dropped to `r percent(1 - source_distr[1])`.

`r english::as.english(n_review_rounds) %>% str_to_title()` CR rounds were necessary to complete the second session classification, with just `r sum(session_summary[-1, 'Unlab. -> y'])` new positive found after reviewing `r tot_reviewed - session_summary$tot_reviewed_[1]` extra records. It is interesting to notice that the first CR round required the user to review a substantial number of records (1273), but just labelling 275 of them (the canonical 250 plus 25 that were already labelled during the framework performance evaluation) was sufficient to drop this number to just 190 in the subsequent round. An evaluation of the convergence (Figure 1, Suppl. Mat. S2) showed that, in addition to the dynamics already observed in session 1 (shrinkage and negative shift), a second mode appeared in the mixture distribution of the records to be reviewed, centred in a highly positive zone. The interpretation is that as the number of negative training records increases, the engine gets more and more sceptical and asks to review even some records labelled as positive in the initial labelling at the beginning of session 1. This behaviour can be useful to spot classification errors and inconsistencies.
Considering both sessions, `r tot_reviewed` records were reviewed and `r num_last_positives` (`r percent(num_last_positives / tot_reviewed)`) were found.

Again, the evaluation of the relative importance of the terms showed that the engine was quite capable of internalizing the concepts behind the research topic. A subsample of these terms is reported in Table 2 of Suppl. Mat. S2.


### Hyperparameter selection and performance evaluation

```{r Performance}

grid_search <- analyse_grid_search()

# Creating these summary is quite demanding, so it's better to cache them
# explicitly
perf_file <- file.path('Manuscript', 'Performance_summary.rds')

if (!file.exists(perf_file)) {
	Performance <- list(
		s1 = get_session_files('Session1')$Annotations %>% last() %>% import_data() %>% estimate_performance(),
		s2 = get_session_files('Session2')$Annotations %>% last() %>% import_data() %>% estimate_performance(),
	)
	
	write_rds(Performance, perf_file)
	
} else {
	Performance <- read_rds(perf_file)
}

perf_table <- format_performance(Performance$s1, Performance$s2)

#perf_s1$plot + guides(color = 'none', fill = 'none') + ggtitle('Session 1') + perf_s2$plot + ggtitle('Session 2')
```

As described in the methods, the selection of hyperparameters <!-- develop more in the methods, the data is not a random sample and it's ordered so performance evaluation needs to take this into account. also note that hyperparameter selection is necessary since even if bayesian methods are robust, the other parameters outside it are not--> was achieved via the evaluation of sensibility and efficiency with a grid search on a subset of 1200 completely manually labelled records (validation set). The best set of parameters suggested an initial input of 250 labelled records, an averaged ensemble of 10 models, no bootstrapping and an uncertainty zone defined by the 98% predictive interval. On the validation set, this combination of parameters reached a sensitivity of `r grid_search$Sensitivity` (`r grid_search$Pos_labels` positive matches found) and efficiency of `r grid_search$Efficiency` (`r grid_search$Tot_labeled` records evaluated). A summary of the results of the grid search is reported in Table 3 in Suppl. Mat. S2.

To evaluate the theoretical performance of the engine on the full datasets (i.e., session1 and session2 data), a Bayesian logistic model was trained to predict the label of the records from the probability estimated by the engine (see methods for details). The performance of such simple model is quite high (Bayesian R2: `r Performance$s1$mod_r2 %>% format_interval(percent = TRUE)` for session 1 and `r Performance$s2$mod_r2 %>% format_interval(percent = TRUE)` for session 2). The predicted cumulative number of positive matches was used to evaluate the performance in the non-reviewed records (Table 3).\

Figure 1 shows the actual and predicted (from the logistic model) cumulative number of positive matches, ordered by the initial simple ordering query. As confirmed by the high efficiency values reported in Table 3, it is striking how many more records one would have needed to evaluate manually to find all positive matches without using a smart search tool. Our engine was able to find matches even close to the end of the heuristically ordered list of records.
Specifically, in session 1 we observe an expected total number of positives of `r Performance$s1$pred_positives %>% format_interval()` for an estimated sensitivity of `r Performance$s1$sensitivity %>% format_interval(percent = TRUE)` and efficiency of `r Performance$s1$sensitivity %>% format_interval(percent = TRUE)`.\
In session 2 we observed a drop in the expected sensitivity, especially in the lower margin (`r Performance$s2$sensitivity %>% format_interval(percent = TRUE)`), due to the fact that as the number of records grows very large, even a small probability can translate, in the worst scenario, into a relevant number of predicted positive matches (`r Performance$s2$pred_positives %>% max()` in this case). To ascertain that no evident positives were missed, we evaluated 100 more records between the unreviewed ones with the highest median predicted probability produced by the engine and found no actual positive matches.

`r knitr::kable(perf_table, caption = 'Table 3.')`

```{r Performance plot, fig.width=12, fig.height=5}
Performance$s1$plot +
	guides(color = 'none', fill = 'none') +
	ggtitle('Session 1') +
	Performance$s2$plot +
	ggtitle('Session 2')
```
