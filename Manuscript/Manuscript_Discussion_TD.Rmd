---
title: "Discussion"
date: "`r lubridate::today()`"
bibliography: references.bib
csl: apa.csl
output:
  pdf_document:
    includes:
      in_header: header.tex
  github_document: default
---

## Discussion

We propose a new integrated framework to help researcher collect and screen scientific publications characterized by high performance and versatility, joining the growing field of systematic review automation (SRA) and helpers (SRH) tools. By sharing the tool as an open source R package and following a modular design of the tool, we tried to adopt some of the so called Vienna Principles advocated by the International Collaboration for the Automation of Systematic Reviews (ICASR) [@beller2018making]. Our tool thus consists of three main components: 1) an integrated query-based citation search and management engine, 2) an active machine learning based citation screener, and 3) a data-driven query generation algorithm.\

The framework's search engine module is capable of automatically collecting citation data from three well known scientific databases (i.e. Pubmed, Web of Science, and the database of the Institute of Electrical and Electronics Engineers) as well as process manually downloaded results from both the mentioned and other (SCOPUS, EMBASE) databases. The online search algorithm is efficient enough to manage tens of thousands of search results, using various expedients to overcome the online databases limitations in terms of traffic and download quota.\

Most SRH tools we are aware of, commercial or free to use, rely either on internal databases (e.g., Mendeley https://www.mendeley.com/) that focus on a particular topic [@visser2010performing] or rely on a single external data source [@thomas2007eppi; @poulter2008mscanner; @soto2019thalia]. To our knowledge, our tool is the first capable to fully automatize searching on three different well known scientific databases and to import raw data from five of them. Mixing different database is fundamental to have a more complete view of the literature on a topic [@bajpai2011search; @wilkins2005embase; @woods1998medline]: in our results, 18.7% of the positive matches were unique for one of the different data sources and no positive record was present among all of them (data not shown).\ 

We improved the transformation of textual data into actionable numeric inputs [@ananiadou2006text; @cohen2008getting] by using network analysis to identify associated terms, labelled as non-consecutive ngrams, conferring particular meaning when present together into a text while not necessary being close to each other. A similar network approach was used to merge redundant terms, in order to make model estimation more efficient and reduce possible noise. The use of network inspired modeling of text is not new [@rousseau2015graph; @violos2016sentiment; @rousseau2015text] and is a valuable tool to extract semantic information not evident in one-word of consecutive ngram models.\

The framework's record screening module implements active machine learning based classification, bringing improvements at various levels. At the base of our classification algorithm we use BART [@chipman2010bart; @kapelner2013bartmachine], a Bayesian machine learning technique characterized by exceptional predictive performance and speed even in large dataset. This Bayesian approach explores distributions of decision trees which are justified by the data [@hastie2009boosting], while using a set of prior distributions to avoid overfitting by discouraging excessively deep trees, long sequences of trees, or extreme predicted probabilities.\

To further improve generalizability we ran multiple replications of the model and averaged their record predictive distributions; this technique, known as ensembling, has been shown to improve out-of-sample predictive performance []. In a Bayesian setting, this effect comes from a reduction of the probability mass in the tails of the predictive distributions while not shifting their locations (i.e., it decreases variance without impacting bias). Instead, imposing stronger general priors against extreme predictions would decreased variance but also shifted the distribution mass towards a non-decision zone, therefore increasing bias.\

Using more then one model replica greatly improved sensitivity and efficiency, with a performance stabilization after 10 replications (Suppl. Mat. S2); since the number of model replications greatly impacted computation times we decided to use ten replicas since it was also associated with the higher sensitivity in the same performance cluster. The variability between models derives only from the randomness in the MCMC fitting phase []; an alternative was to impose additional variability by bootstrapping [] but this approach decreased performance (Suppl. Mat. S2), probably due to the low proportion of positive matches in the data and the loss of information with bootstrapping in unbalanced classification tasks [].\

Given that our framework can (automatically) collect a vast amount of records, most records will be non-relevant (labelled negative). This strong imbalance between positive and negative records [@sampson2011precision] can affect sensitivity []. To overcome the problem, we oversampled the positive records ten times before model fitting. Our hyperparameter analysis showed that together with model ensembling, the oversampling rate was the parameter with the highest impact on performance. A known risk with oversampling of the target class is the missclassification of negative records as positive [], but since in our approach all predicted positive are manually reviewed we are ensured to achieve always 100% specificity/positive predictive value. An alternative approach would be record weighting to change error costs during estimation [] but the BART implementation we used did not allow observation weighting and using simple oversampling allows to use different modeling engines. Furthermore, the simple query ordering of the record during the record collection allowed for a far higher prevalence of relevant records in the initial training set (17.2%) compared to the overall one (0.11%) and this boosts the overall sensitivity of the model.\

At every iteration the researcher is asked to manually review a number or records, defined by the size of the uncertainty zone, which is dynamically shrinking (Suppl. Mat. S2 Fig. 1). This uncertainty zone is identified by joining the distribution of probabilities produced by exploiting the Bayesian nature of BART for both the positive and negative record groups. The researcher therefore doesn't need to choose a priori the number of records to review but just the size of the uncertainty zone (by setting the PPD quantiles used to built it) and after how many iterations with no new positive matches to stop. This is where our approach offers a major innovation in overcoming of simple stopping rules in SRA classification tasks []. Since our approach requires researchers to review both unlabelled records with a positive predicted label and those with a PPD inside the uncertainty zone, it can be considered as a unifying synthesis of the "certainty" and "uncertainty" paradigms of active learning [@miwa2014reducing].\

To evaluate performance we avoided the classic random out-of-sample approaches like train-test sampling, out-of-bag bootstrapping or cross-validation [@kohavi1995study] because we deemed them not realistic in this setting. It is instead more relevant to estimate the long run sensitivity of our prediction method in finding all relevant records in the whole data set. Since the records are ordered, again a random approach would not work since it assumes that the prevalence of positive records is equal on average in every possible sample [@tashman2000out], while instead it drops as more records are considered (Fig. 2). We instead used a Bayesian model to estimate the likelihood of a positive match given the prediction scores of the BART model. This approach showed to fit the data very well (high $R^2$) and the Bayesian predictive distribution can be exploited to draw a worse case scenario given the data.\

Our framework achieved a very high sensitivity screening just 1.34% of all downloaded records, a sensible reduction in workload. The approach reached 98.8% sensitivity during the hyperparameter optimization (deterministic, small sample) and on the whole data set we predicted a theoretical sensitivity of 100% [93.5%, 100%] in the first session and of 97.3% [73.8%, 100%] in the second, a lower value as is expected by the probabilistic accumulation of random events. Both results are above the usual results in the field [@o2015using] and in line with the 92% average sensitivity estimated after human only screening [@edwards2002identification]. In one interesting case, the model spotted a misclassification during initial labeling, demonstrating its robustness and its value also as a second screener, as already suggested by previous studies [@frunza2010building; @bekhuis2012screening; @bekhuis2010towards]. Finally, albeit the simple query based ordering did concentrate most of the relevant matches in the first 20-25 thousands records, some relevant records would have required almost the full data set to be manually checked to be found.\

The model takes ~5-20 minutes per iteration to perform predictions in session 1 (17755 documents) and 20-40 minutes in session 2 (98371 documents) on a 8 core, 2.5 GhZ, 16 GB RAM laptop from 2014 and that equate to 1-3 days per session to manually review and perform predictions, for a total of 1-2 weeks for the whole process, a huge saving of time compared to the months usually required for the screening phase of systematic reviews [@bannach2019machine; @borah2017analysis; @allen1999estimating]. To our knowledge, our data sets are larger than what common in most SRA studies [@o2015using; @olorisade2016critical], emphasizing the reliability of the tool in real world scenarios.\

The last module of our framework is a data-driven query generation algorithm. The creation of an efficient and efficacious search query is a complex task [@lefebvre2011searching; @hammerstrom2010searching] since it requires building a combination of positive and negative terms to maximize the number of relevant search results while minimizing the total number of results. We propose a solution based on concurrent decision trees [@blanco2019machine; @moore2018transparent] built on the posterior predicted probabilities; the technique tries to extract high sensitivity subqueries from the labeled data, enrich them with negative terms to increase specificity [@abdelmgeid2008using] and then let researcher evaluate them and pick the most meaningful ones. The framework will then join them and remove redundancies. The aim is to generate a second query that complement the first human made one and help find possible missing records not found during the first session.\

The generated query allowed to retrieve few more positive matches not found in session 1 at the cost of a large increase in the amount of documents. One interesting aspect of this functionality is that it provides a human-readable overview of the classification rules internalized by the classification model, showing which combination of terms was particularly relevant and even spotting authors and locations associated to the topic of study. The generated query therefore acted as a tool for machine learning explainability [@bhatt2020machine; @burkart2021survey], a feature useful to spot bias in black box classification algorithms [@malhi2020explainable] and that is increasingly legally required for high-stack machine learning applications [@bibal2021legal; @bibal2020impact].\

It is important to note that this process is entirely data-driven. The algorithm is only aware of the "world" defined by the data set, itself generated by a specific search query focused on a particular topic. Therefore, the new query may not be specific once applied to an unbounded search domain, returning an unmanageable amount of unrelated results. The solution we found was to add another component specifying the general topic (antimicrobial resistance and healthcare associated infections) of our research.\

As reported, our framework builds on modularity. We designed it in order to easily implement complete independency of the main modules in future iterations. In our view it would be possible to easily add custom citation search and parsing functions for other scientific databases, alternative text features building algorithms and alternative machine learning modules. We deem such interoperability extremely relevant, because the main strength of our tool is the composition of many solutions and the general approach related to Bayesian active machine learning, but each of its components could benefit greatly from the recent improvements in text mining. For example, our text feature generation approach is based on the boolean bag-of-words paradigm, and surely it could be improved by more nuanced text representations: it could be evaluated if feature transformations like Tf-Idf [@baeza1999modern; @ananiadou2006text], would be advantageous, even if we hypothesize that tree based classification algorithms like BART are robust enough to not need such operation. Word embedding could be considered, which transform terms in semantic vectors based on the surrounding text [@turian2010word; @bollegala2015embedding, @minaee2021deep], and could be used to eliminate semantically redundant terms or differentiate identical terms with different meanings given the context; another option would be to use unsupervised learning models like Latent Dirichlet Analysis, Latent Semantic Analysis, etc., [@pavlinek2017text; @chen2016short; @landauer1998introduction] to extract topics to enrich the feature space.\

Our classification algorithm can be implemented with any Bayesian supervised machine learning method which produces full PPDs; therefore alternative models could be evaluated, like Gaussian Processes which are known for their flexibility [@jayashree2020evaluation; @chen2015gaussian]. Even more interesting would be to test  advanced learning algorithms that surpass the bag-of-words approach, taking into consideration higher level features in the text like term context and sequences, long distance term relationships, semantic structures, etc., [@cheng2019document; @minaee2021deep; @li2020survey; @yang2020survey; @lai2015recurrent; @farkas1995document], given that a Bayesian implementation of such algorithms is available (for example @chen2018approximate).\

