%% BioMed_Central_Tex_Template_v1.06
%%                                      %
%  bmc_article.tex            ver: 1.06 %
%                                       %

%%IMPORTANT: do not delete the first line of this template
%%It must be present to enable the BMC Submission system to
%%recognise this template!!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                     %%
%%  LaTeX template for BioMed Central  %%
%%     journal article submissions     %%
%%                                     %%
%%          <8 June 2012>              %%
%%                                     %%
%%                                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% For instructions on how to fill out this Tex template           %%
%% document please refer to Readme.html and the instructions for   %%
%% authors page on the biomed central website                      %%
%% https://www.biomedcentral.com/getpublished                      %%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%% BioMed Central currently use the MikTex distribution of         %%
%% TeX for Windows) of TeX and LaTeX.  This is available from      %%
%% https://miktex.org/                                             %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% additional documentclass options:
%  [doublespacing]
%  [linenumbers]   - put the line numbers on margins

%%% loading packages, author definitions

%\documentclass[twocolumn]{bmcart}% uncomment this for twocolumn layout and comment line below
\documentclass[]{bmcart}

%%% Load packages
\usepackage{amsthm,amsmath}
\RequirePackage[numbers]{natbib}
%\RequirePackage[authoryear]{natbib}% uncomment this for author-year bibliography
%\RequirePackage{hyperref}
\usepackage[utf8]{inputenc} %unicode support
%\usepackage[applemac]{inputenc} %applemac support if unicode package fails
%\usepackage[latin1]{inputenc} %UNIX support if unicode package fails
\usepackage[strings]{underscore} % CUSTOM: fixes problems with underscores in .bib

\usepackage{caption}
\usepackage{placeins}
\usepackage{amsmath}
\usepackage{lineno}
%\linenumbers
\captionsetup[table]{labelformat=empty}
\captionsetup[figure]{labelformat=empty}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                             %%
%%  If you wish to display your graphics for   %%
%%  your own use using includegraphic or       %%
%%  includegraphics, then comment out the      %%
%%  following two lines of code.               %%
%%  NB: These line *must* be included when     %%
%%  submitting to BMC.                         %%
%%  All figure files must be submitted as      %%
%%  separate graphics through the BMC          %%
%%  submission process, not included in the    %%
%%  submitted article.                         %%
%%                                             %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \def\includegraphic{}
% \def\includegraphics{}

%%% Put your definitions there:
\startlocaldefs
\def\url{} % CUSTOM: \url requires hyperref package, but loading it causes errors with the author formatting

\providecommand{\tightlist}{ % CUSTOM: pandoc uses \tightlist for lists, without loading the required package
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
  }
\endlocaldefs

%%% Begin ...
\begin{document}

%%% Start of article front matter
\begin{frontmatter}

\begin{fmbox}
\dochead{Research}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the title of your article here     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{An open-source integrated framework for the automation of
citation collection and screening in systematic reviews.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors here                   %%
%%                                          %%
%% Specify information, if available,       %%
%% in the form:                             %%
%%   <key>={<id1>,<id2>}                    %%
%%   <key>=                                 %%
%% Comment or delete the keys which are     %%
%% not used. Repeat \author command as much %%
%% as required.                             %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author[
  addressref={aff1},
    corref={aff1},
    email={angelo.d.ambrosio@uniklinik-freiburg.de,a.dambrosioMD@gmail.com}
]{\inits{A.D.}\fnm{Angelo} \snm{D'Ambrosio}}
\author[
  addressref={aff1},
    email={hajo.grundmann@uniklinik-freiburg.de}
]{\inits{H.G.}\fnm{Hajo} \snm{Grundmann}}
\author[
  addressref={aff1},
    email={tjibbe.donker@uniklinik-freiburg.de}
]{\inits{T.D.}\fnm{Tjibbe} \snm{Donker}}

% \author[
%   addressref={aff1},                   % id's of addresses, e.g. {aff1,aff2}
%   corref={aff1},                       % id of corresponding address, if any
% % noteref={n1},                        % id's of article notes, if any
%   email={jane.e.doe@cambridge.co.uk}   % email address
% ]{\inits{J.E.}\fnm{Jane E.} \snm{Doe}}
% \author[
%   addressref={aff1,aff2},
%   email={john.RS.Smith@cambridge.co.uk}
% ]{\inits{J.R.S.}\fnm{John R.S.} \snm{Smith}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors' addresses here        %%
%%                                          %%
%% Repeat \address commands as much as      %%
%% required.                                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\address[id=aff1]{
    \orgdiv{Institute for Infection Prevention and Hospital Hygiene},
      \orgname{Freiburg University Hospital},
          \city{Freiburg},
      \cny{Germany}
  }

% \address[id=aff1]{%                           % unique id
%   \orgdiv{Department of Science},             % department, if any
%   \orgname{University of Cambridge},          % university, etc
%   \city{London},                              % city
%   \cny{UK}                                    % country
% }
% \address[id=aff2]{%
%   \orgdiv{Institute of Biology},
%   \orgname{National University of Sciences},
%   %\street{},
%   %\postcode{}
%   \city{Kiel},
%   \cny{Germany}
% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter short notes here                   %%
%%                                          %%
%% Short notes will be after addresses      %%
%% on first page.                           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{artnotes}
%%\note{Sample of title note}     % note to the article
%\note[id=n1]{Equal contributor} % note, connected to author
%\end{artnotes}

 \end{fmbox} % comment this for two column layout

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                           %%
%% The Abstract begins here                  %%
%%                                           %%
%% Please refer to the Instructions for      %%
%% authors on https://www.biomedcentral.com/ %%
%% and include the section headings          %%
%% accordingly for your article type.        %%
%%                                           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstractbox}

\begin{abstract} % abstract
\parttitle{Background} %if any
The exponential growth of scientific production makes secondary
literature abridgements increasingly demanding. We introduce a new
open-source framework for systematic reviews that significantly reduces
time and workload for collecting and screening scientific literature.
\parttitle{Methods} %if any
The framework provides three main tools: 1) an automatic citation search
engine and manager that collects records from multiple online sources
with a unified query syntax, 2) a Bayesian, active machine learning,
citation screening tool based on iterative human-machine interaction to
increase predictive accuracy and, 3) a semi-automatic, data-driven query
generator to create new search queries from existing citation data
sets.\\
To evaluate the automatic screener's performance, we estimated the
median posterior sensitivity and efficiency {[}90\% Credible
Intervals{]} using Bayesian simulation to predict the distribution of
undetected potentially relevant records.\\
\parttitle{Results} %if any
Tested on an example topic, the framework collected 17,755 unique
records through the citation manager; 766 records required human
evaluation while the rest were excluded by the automatic classifier; the
theoretical efficiency was 95.6\% {[}95.3\%, 95.7\%{]} with a
sensitivity of 100\% {[}93.5\%, 100\%{]}.\\
A new search query was generated from the labelled dataset, and 82,579
additional records were collected; only 567 records required human
review after automatic screening, and six additional positive matches
were found. The overall expected sensitivity decreased to 97.3\%
{[}73.8\%, 100\%{]} while the efficiency increased to 98.6\% {[}98.2\%,
98.7\%{]}.\\
\parttitle{Conclusions} %if any
The framework can significantly reduce the workload required to conduct
large literature reviews by simplifying citation collection and
screening while demonstrating exceptional sensitivity. Such a tool can
improve the standardization and repeatability of systematic reviews.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The keywords begin here                  %%
%%                                          %%
%% Put each keyword in separate \kwd{}.     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{keyword}
	\kwd{Systematic review automation}
	\kwd{Citation management}
	\kwd{Online data collection}
	\kwd{Active machine learning}
	\kwd{Natural language processing}
	\kwd{Bayesian modeling}
% \kwd{sample}
% \kwd{article}
% \kwd{author}
\end{keyword}

% MSC classifications codes, if any
%\begin{keyword}[class=AMS]
%\kwd[Primary ]{}
%\kwd{}
%\kwd[; secondary ]{}
%\end{keyword}

\end{abstractbox}
%
 %a  \end{fmbox} %uncomment this for two column layout

\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                            %%
%% The Main Body begins here                  %%
%%                                            %%
%% Please refer to the instructions for       %%
%% authors on:                                %%
%% https://www.biomedcentral.com/getpublished %%
%% and include the section headings           %%
%% accordingly for your article type.         %%
%%                                            %%
%% See the Results and Discussion section     %%
%% for details on how to create sub-sections  %%
%%                                            %%
%% use \cite{...} to cite references          %%
%%  \cite{koon} and                           %%
%%  \cite{oreg,khar,zvai,xjon,schn,pond}      %%
%%                                            %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%% start of article main body
% <put your article body there>

%%%%%%%%%%%%%%%%
%% Background %%
%%

\section*{Background}
\addcontentsline{toc}{section}{Background}

Scientific production has experienced continuous exponential growth in
the last decades \citep{larsen2010rate, bornmann2015growth}. This is
especially true for biomedical research, a trend further accelerated by
the COVID-19 pandemic, thanks to faster article processing time by
publishers and the greater use of preprint databases
\citep{aviv2021publication, horbach2020pandemic, hoy2020rise}.
Consequently, it has become harder for researchers and practitioners to
stay up to date on the latest findings in their field. Secondary
research is of paramount importance in this scenario in that it provides
valuable summaries of the latest research results; however, it is
becoming ever more challenging in terms of time and human resources
required
\citep{allen1999estimating, borah2017analysis, cohen2010evidence, bastian2010seventy}.\\
The article collection and screening phases of a systematic review are
particularly demanding \citep{babar2009systematic}. First, relevant
published research must be collected from scientific databases using
appropriately built search queries (retrieval phase); secondly, the
scientific citations collected must be screened, selecting only those
that are relevant to the topic (appraisal phase)
\citep{bannach2019machine, tsafnat2014systematic, higgins2019cochrane}.\\
Search queries construction is a complex task
\citep{lefebvre2011searching, hammerstrom2010searching}, requiring both
expertise in the scientific field of interest and some knowledge of the
database query languages. The goal is to obtain a set of results that
contains all relevant articles (high sensitivity) while keeping the
total number of records low (high specificity), possibly focusing on the
first at the expense of the second \citep{hammerstrom2010searching}.\\
If an integrated search tool is not used, manual work is required to
download, store and organise the publication data; this approach is
complicated by limits to the number of records that can be downloaded at
any one time and the need to harmonise different formats and resolve
record duplication \citep{marshall2015systematic}.\\
The citation screening phase is usually the more resource-demanding task
in a systematic review: even with appropriately built search queries,
the results may easily range in the tens of thousands, of which just a
small fraction are actually relevant \citep{lefebvre2011searching}. It
has been estimated that labelling 10,000 publications can take up to 40
weeks of work and that the average clinical systematic review takes 63
weeks to complete
\citep{bannach2019machine, borah2017analysis, allen1999estimating}. A
consequence of this is that systematic reviews are often already
out-of-date by the time they are published
\citep{beller2013systematic}.\\
The field of Data Science applied to evidence synthesis and acquisition
has greatly maturated in the last years
\citep{marshall2015systematic, beller2018making, tsafnat2014systematic}.
By applying natural language processing (NLP), it is possible to
transform free text into quantitative features, with various levels of
abstraction and generalisation
\citep{ananiadou2006text, cohen2008getting}; using machine learning,
such text-derived data can be used to map and reproduce human judgment,
automating the screening of citations \citep{ikonomakis2005text}.\\
Automation of systematic reviews has made significant improvements in
the last years
\citep{ananiadou2009supporting, o2015using, tsafnat2013automation, jonnalagadda2015automating},
and it is possible foreseeable that it will become the standard approach
in the field \citep{beller2018making}, with many solutions already being
implemented into commercial or free-to-use tools \citep[see][table
1]{marshall2015systematic}.\\
This manuscript introduces an open-source, production-ready framework
that further contributes to the state-of-the-art in systematic review
automation (SRA) and helpers (SRH) tools. We improve the ``retrieval
phase'' by providing a unified framework for the automated collection
and management of scientific literature from multiple online sources.
For the citation screening (appraisal) phase, we built an active machine
learning-based protocol \citep{settles2009active, miwa2014reducing},
which utilises a Bayesian framework to efficiently identify potentially
relevant documents that require human review while automatically
screening-out the vast majority of clearly non-relevant ones; the
algorithm then requires human review to increase classification accuracy
iteratively. Finally, we included a tool to generate new search queries
based on an already categorised citation data set, to identify relevant
research that manually-made queries may have possibly missed.\\
We tested the framework in the retrieval and appraisal phases of an
example topic of interest to our group: the evaluation of the
mathematical modelling of patient referral networks among hospitals and
their impact on the diffusion of healthcare-associated pathogenic
microorganisms; the protocol is published in \citep{newis}.\\
In the Methods, we give an overview of the framework, in the Result, we
show the outputs and performance of the framework applied to the example
topic, and in the Discussion, we explain the methodological rationale
for the different components and features of the framework.\\

\section*{Methods}
\addcontentsline{toc}{section}{Methods}

\subsection*{General description}
\addcontentsline{toc}{subsection}{General description}

We built an R \citep{rstats2021} based framework to simplify two aspects
of systematic literature review: record acquisition and classification.
The code used to generate the results is available at
https://github.com/AD-Papers-Material/BART\_SystReviewClassifier, while
an updated and ready to use version of the framework is distributed as
an R package at https://github.com/bakaburg1/BaySREn. The framework
includes several modules that communicate through intermediate outputs
stored in standard formats, making it possible for users to extend the
framework or easily integrate it with other tools in their pipeline. See
Supplemental Material S1 for an in-depth description of the framework
and how to use it.\\
The tasks carried out by the framework are grouped into ``sessions'',
which comprise obtaining scientific citation data (records) using a
search query and then labelling them as relevant (``positive'' in the
rest of the text) or not (``negative'') for the topic of interest with
the help of a machine learning engine (Fig. 1). The initial search query
should be built using domain knowledge, trying to achieve a high
relevant/non-relevant record ratio.\\
The framework can then generate a new data-driven query from this
labelled set to perform a new session to find records possibly missed by
the first query.\\

\subsection*{Record acquisition and initial labelling}
\addcontentsline{toc}{subsection}{Record acquisition and initial
labelling}

We built a set of tools to allow users to automatically search and
download citation data from three major scientific databases
(``sources''): Pubmed (\url{https://pubmed.ncbi.nlm.nih.gov/}), Web Of
Science (WOS, \url{https://apps.webofknowledge.com/}) and the Institute
of Electrical and Electronics Engineers (IEEE,
\url{https://ieeexplore.ieee.org/Xplore/home.jsp}). The framework
handles authorisation management for non-open databases like WOS and
IEEE. It is also possible to import previously downloaded records in the
framework; this is particularly useful for acquiring records from SCOPUS
(\url{https://www.scopus.com/}) and EMBASE databases
(\url{https://www.embase.com/}), for which a comprehensive API interface
was not easy to build. An extra manual search was also necessary for
Pubmed since the API and the web interface have different rule expansion
algorithms and return slightly different results \citep{pubmedUpdate}. A
short guide on how to set up the framework for each database supported
is available in Supplemental Material S3.\\
The collected records are merged into a single database, resolving
duplicates and different formatting between sources. The records are
ordered according to the frequency of the positive query terms (e.g.,
not preceded by a \emph{NOT} modifier) in the title and abstract
(``simple query ordering'').\\
The researcher is then asked to label a subset of records to create the
``initial training set'' needed to start the automatic classification.
We recommend manually labelling the first 250 records (see
``hyperparameter optimisation'' later). Simple query ordering increases
the positivity rate in the initial training set
\citep{wallace2010active}, leading to higher sensitivity during
automatic classification \citep{chawla2004special}.

\subsection*{Text feature extraction}
\addcontentsline{toc}{subsection}{Text feature extraction}

The framework models the relevance of a record based on the following
fields in the citation data: title, abstract, authors, keywords, MeSH
terms \citep{lipscomb2000medical}. A range of Natural Language
Processing (NLP) techniques
\citep{baeza1999modern, marshall2015systematic, ananiadou2006text} are
employed to convert the textual information in these fields into
features for machine learning through a bag-of-words approach
\citep{marshall2015systematic}. Processing of free text fields (title,
abstract) includes: tokenisation (i.e., extracting the terms), removal
of common stopwords (i.e., sentence components having no semantic
value), part-of-speech filtering (only nouns, adjectives, verbs and
untagged terms are retained), and lemmatisation of terms (i.e.,
reduction to their base grammatical form). Text processing for authors,
keywords and MeSH terms identifies logical units (e.g., authors' full
names, composite keywords) and extracts them.\\
Terms appearing in less than 5\% of the labelled documents are removed
from negative records. All terms in the positive set are kept to
increase sensitivity at the cost of specificity.\\
Some terms tend to co-appear in records (non-consecutive ngrams,
nc-ngrams), often carrying a particular meaning when they do co-occur.
To detect nc-ngrams, we generated a word network representation
\citep{rousseau2015graph} with edges occurring between terms with a
cosine similarity in terms of document co-occurrence \textgreater{} 0.5.
We extracted the maximal cliques in the network
\citep{eppstein2010listing} representing highly correlated groups of
terms; these groups are added to the dataset as individual features.
Only nc-ngrams comprising a maximum of ten terms are kept.\\
A second network is built using a co-occurrence threshold of 0.9. In
this case, the cliques represent terms that always appear together and
can therefore be considered redundant (i.e., they do not need to be
considered separately). These terms are merged to increase computational
efficiency and reduce overfitting.\\
The output is a Document-Term Matrix (DTM), with \(N_d\) rows
representing the records (\(D_i\)), \(N_t\) terms column for the
\(t_{field}\) terms (divided by record field) and \({0,1}\) values
whether \(t_{field} \in D_i\). We also enriched the DTM with features
referencing the number of terms in each field to help the model scale
term importance based on the field length.

\subsection*{Label prediction}
\addcontentsline{toc}{subsection}{Label prediction}

We used a Bayesian Additive Regression Trees (BART) machine learning
``classification model'' \citep{chipman2010bart} \citep[in the
implementation of][]{kapelner2013bartmachine} to predict the probability
of a record being relevant, given the information coded into the
enriched DTM and the initial training set. We set up the BART model to
use 2,000 MCMC iterations (after 250 burn-in iterations) and 50 trees;
we used a \(k\) value of 2 to regularise extreme prediction and let the
model use missing fields in the DTM as features
\citep{kapelner2015prediction}. Positive records are oversampled ten
times to increase sensitivity \citep{batista2004study}.\\
The output is the expected value posterior predictive distribution for
each record (PPD in brief) describing the probability of it being
relevant (i.e., a positive match). An ensemble of ten models was fitted
to improve prediction stability by averaging the PPD between models
\citep{zhou2021ensemble, dietterich2000ensemble}.\\

To assign the labels, we employed an ``active learning'' approach
\citep{settles2009active, miwa2014reducing}, where a human reviews a
specific subset of predictions made by the machine, which is then
retrained on the manually reviewed dataset. This process is carried out
iteratively to reduce prediction uncertainty.\\
Label assignment is done through identification of an ``uncertainty
zone'', the construction of which is possible thanks to the Bayesian
nature of BART, which provides full PPDs instead of point-wise
predictions for each record.\\
To describe the process formally, we define:

\[\pi_i = \frac{1}{M}\sum_{j=1}^M Pr(L_i = \text{1}|DTM,m_j) \tag{Eq. 1}\]

as the PPD of a record \(D_i\) being relevant (i.e, having a positive
label, \(L_i = 1\)), averaging the PPDs of the ensemble of \(M=10\)
models \(m\), and:

\[
\begin{aligned}
\pi_{i,l} = \{\pi_i : Pr(\pi_i) = 1\%\}\\
\pi_{i,u} = \{\pi_i : Pr(\pi_i) = 99\%\}
\end{aligned}
\tag{Eq. 2}
\]

respectively as the lower and upper boundaries of the 98\% quantile
interval of \(\pi_i\) (98\% Predictive Interval, 98\% PrI).\\
Then we identify the ``uncertainty zone'' as:

\[U_\pi=[\max\vec{\pi}_{u}^-, \min\vec{\pi}_{l}^+] \tag{Eq. 3}\]

with \(\vec{\pi}_{u}^-\) being the vector of \(\pi_{i,u}\) with a
negative label and \(\vec{\pi}_{l}^+\) the vector of \(\pi_{i,l}\) with
a positive label. That is, \(U_\pi\) defines a range of values between
the smallest \(\pi_{i,l}\) in the set of already labelled positive
records \(L_p\) and the largest \(pi_{i,u}\) related to the negative
ones \(L_n\), noting that the two limits can appear in any order.\\
Consequently, a record \(D_i\) will be labelled as positive if:

\[\pi_{i,l} > \max_{\pi \in U_\pi} \pi \tag{Eq. 4}\]

that is, the record lower 98\% PrI boundary should be higher than every
value in the uncertainty zone. In other words, for a record to be
labelled positive, its PPD should be within the range of the mixture of
PPD of the records previously labelled positive and should not cross the
distributions of the negative records.\\
Conversely, a record is labelled as negative if:

\[\pi_{i,u} < \min_{\pi \in U_\pi} \pi \tag{Eq. 5}\]

The remaining records are labelled as ``uncertain''.

Manual review is then necessary for: 1) uncertain records, 2) positive
records (to avoid false positives), and 3) records whose predicted label
differs from the existing manual one. The last case helps identify human
errors or inconsistent labelling criteria.

The automatic classification and manual review steps alternate in a loop
(CR iterations) until no new positive matches are found in four
consecutive iterations.

\subsection*{Relevant term extraction}
\addcontentsline{toc}{subsection}{Relevant term extraction}

As a measure of feature importance, we computed the ``inclusion rate'',
that is, the proportion of times a term is used in a posterior tree over
the sum of total inclusions of all variables
\citep{kapelner2013bartmachine}. We extracted the terms, the portion of
the citation data in which they were used, the average ``inclusion
rate'' among the ensemble models (over 10,000 inclusions) and its ratio
over the standard deviation of this inclusion (inclusion stability, IS).
For each term, we ran a Poisson regression to get the linear association
with a positive label and reported it as Relative Risk (RR) with the
number of standard errors as significance index (Statistic); the
comparison between the inclusion rate in the BART models and the linear
association allows to spot relevant non-linear effects (i.e., the
feature is relevant only in association with others). In the Results, we
only listed the first 15 terms with IS \textgreater{} 1.5 (in order of
inclusion rate), while the first fifty terms, regardless of inclusion
stability, are listed in Supplemental Material S2.

\subsection*{New search query generation}
\addcontentsline{toc}{subsection}{New search query generation}

We developed an algorithm that generates a new search query to find
further relevant publications missed in the initial search, possibly at
a reasonable cost to specificity (i.e., a higher number of negative
results).\\
The algorithm is composed of the following steps:

\begin{itemize}
\tightlist
\item
  A partition tree \citep{rpart} is built between the DTM and 800
  samples from the PPD; if a term is present multiple times in the DTM
  (e.g., both in the title and abstract), it is counted just once, and
  field term count features are removed. This step generates a list of
  rules composed by \emph{AND}/\emph{NOT} ``conditions'' made of
  terms/authors/keywords/MeSH tokens, which together identify a group of
  records.
\item
  For each rule, negative conditions (i.e., \emph{NOT} statements) are
  added iteratively, starting from the most specific one, until no
  conditions are found that would not also remove positive records.
\item
  The extended set of rules is sorted by positive-negative record
  difference in descending order. The cumulative number of unique
  positive records is computed and used to group the rules. Rules inside
  each group are ordered by specificity.
\item
  The researcher is then asked to review the rule groups and select one
  or more rules from each group or edit overly specific rules (e.g.,
  citing a non-relevant concept casually associated with a paper, like a
  numeric value or indicator). It is possible to exclude a group of
  rules altogether, especially those with the poorest
  sensitivity/specificity ratio.
\item
  The selected rules are joined together by \emph{OR} statements,
  defining a subset of records with a sensibly higher proportion of
  positive records than the original set
\item
  Redundant (i.e., rules whose positive records are already included in
  more specific ones) and non-relevant rules (i.e., conditions that when
  removed do not impact sensitivity and specificity) are removed.
\item
  Finally, the rules are re-elaborated in a query that can be used to
  perform a new citation search.
\end{itemize}

Because the algorithm is data-driven, it creates queries that
effectively select positive records from the input dataset but may be
not specific enough when applied to actual research databases. Therefore
we added an extra subquery in \_AND\_that specifies the general topics
of our search and narrows the search domain.\\
The new query was used to initiate a second search session.

\subsection*{Performance evaluation}
\addcontentsline{toc}{subsection}{Performance evaluation}

We trained a simple Bayesian logistic regression (surrogate model) on
the reviewed records to evaluate the consistency of the classification
model (see Discussion for the theoretical justification). The surrogate
model uses as predictor the lower boundary of the 98\% PrI of the PPD of
the records with weakly regularising, robust priors for the intercept
(Student T with \(\nu=3,\mu=0,\sigma=2.5\)) and the linear coefficient
(Student T with \(\nu=3,\mu=0,\sigma=1.5\)).\\
The quality of the model was evaluated through the Bayesian \(R^2\)
\citep{gelman2019r}, of which we reported the posterior median and 90\%
Credible Interval {[}90\% CrI{]}. The \(R^2\) also provides an
evaluation of the consistency of the original classification model.
Given that this model is conditional only on the BART predictions and
not on the DTM, it is characterised by more uncertainty, providing
plausible worst-case scenarios.\\
The surrogate model is then used to generate the predictive cumulative
distribution of the number of total positive records in the whole
dataset. This distribution allows estimating the expected total
posterior ``Sensitivity'' and ``Efficiency'' of the classification model
in the whole (unreviewed) dataset. Efficiency is summarised by the
``Work saved over random'' (WSoR) statistic: one minus the ratio between
the number of records manually reviewed and those that would be required
to find the same number of positives if classification were performed
choosing records randomly; this last quantity is estimated through a
negative hypergeometric distribution \citep{chae1993presenting} over the
predicted number of positive records.\\
For the number of predicted positive records, sensitivity and
efficiency, we reported the ``truncated 90\% PrI'' {[}trunc. 90\%
PrI{]}, i.e., the uncertainty interval bounded by the number of observed
total positive records (i.e., there cannot be fewer predicted positive
records than observed).

\subsection*{Hyperparameter evaluation}
\addcontentsline{toc}{subsection}{Hyperparameter evaluation}

Our classification algorithm has a limited number of hyperparameters:

\begin{itemize}
\tightlist
\item
  Size of the initial training set: 50, 100, 250, 500 records;
\item
  Number of models in the ensemble: 1, 5, 10, 20, 40, 60 repetitions;
\item
  Oversampling rate of positive records: 1x (i.e., no oversampling),
  10x, 20x;
\item
  PrI quantiles for building the uncertainty zone: 80\%, 90\%, 98\%;
\item
  Source of randomness between models in the ensemble: MCMC sampling
  only \citep{robert2004monte}, MCMC plus data bootstrapping
  \citep{breiman1996bagging} of the training set.
\end{itemize}

To evaluate the hyperparameter effect of performance, we set up a ``grid
search'' \citep{claesen2015hyperparameter, yang2020hyperparameter} on a
prelabelled ``validation set'' derived from the first 1,200 records of
the first session dataset. Each hyperparameter combination was tested
until four CR iterations were completed with no positive records or
until the whole dataset was labelled.\\
For each combination, a performance score was computed as the product of
``Efficiency'' (1 minus the ratio of records that required reviewing
over the total number of records) and ``Sensitivity'' (number of
positive records found over the total number of positive records). We
then used a partition tree \citep{rpart} to identify homogeneous
``performance clusters'' of scores given hyperparameter values. For the
rest of the study, we used the best hyperparameter set in terms of
sensitivity followed by efficiency from the cluster with the highest
average score.\\

\subsection*{Software and data}
\addcontentsline{toc}{subsection}{Software and data}

The framework is built with R v4.0.4 \citep{rstats2021}. The R packages
required by the framework are listed at
https://github.com/bakaburg1/BaySREn/blob/main/DESCRIPTION. All relevant
data necessary to replicate the results is available at
https://doi.org/10.5281/zenodo.6323360.

\section*{Results}
\addcontentsline{toc}{section}{Results}

\subsection*{First session}
\addcontentsline{toc}{subsection}{First session}

The initial search query for the example topic was:\\

\emph{((model OR models OR modeling OR network OR networks) AND
(dissemination OR transmission OR spread OR diffusion) AND (nosocomial
OR hospital OR ``long-term-care'' OR ``long term care'' OR ``longterm
care'' OR ``long-term care'' OR ``healthcare associated'') AND
(infection OR resistance OR resistant))}\\

selecting only results between 2010 and 2020 (included). Results were
collected from Pubmed, WOS, IEEE, EMBASE and SCOPUS, using the framework
tools as described in the Methods and Supplemental Material S1.

The first search session returned a total of 27,600 records,
specifically 12,719 (71.6\% of the total) records from the EMBASE
database, followed by 9,546 (53.8\%) from Pubmed, 3,175 (17.9\%) from
SCOPUS, 2,100 (11.8\%) from WOS, and 60 (0.34\%) from IEEE (Table 1).
There were various degrees of overlapping between sources, with 38.4\%
of records being present in more than one database, and EMBASE and IEEE
being the databases with the higher uniqueness ratios. The final data
set was composed of 17,755 unique records.\\
The first 250 records (based on ``simple query ordering'') were
categorised manually. Of these 43 (17.2\%) were labeled as positive, and
207 (82.8\%) as negative.

The categorised records were used to train the Bayesian classification
model used to label the remaining records. After seven classification
and review (CR) iterations (three resulting in new positive matches and
four extra replications to account for stochastic variability), a total
of 101 positives matches were found, requiring manual review of 766
records (13.2\% positivity rate).\\
It is noticeable how the number of records that required manual review
decreased rapidly between iterations (Table 2), indicating that the
engine was converging while the uncertainties were resolved.\\
This phenomenon is better illustrated in Fig. 1 of Supplemental Material
S2. It shows the mixture distribution of the PPDs of the records,
specifically for records that were manually reviewed, before and after
the classification step: it can be seen how the distribution of
uncertain records shrinks (i.e., it becomes concentrated in a shorter
probability range) and shifts toward the negative zone as more positive
matches are found and reviewed.

We extracted the 15 more relevant terms for the classification model,
described as: Term (citation part): Inclusion Rate (Inclusion Stability)
{[}linear Relative Risk, Statistic{]}:\\

Patient Transport (Keyword): 61.2 (3.77) {[}99.1, 21.3{]}, Transfer
(Abstract): 57 (3.93) {[}22.5, 15.4{]}, Network (Title): 56.5 (2.91)
{[}18, 14.2{]}, Network \& Patient (Abstract): 54.2 (4.66) {[}26.3,
15.2{]}, Donker T (Author): 53.5 (4.56) {[}159, 16.5{]}, Worker
(Abstract): 50 (3.33) {[}0.421, -1.21{]}, Hospitals (Keyword): 49.8
(4.31) {[}27.8, 16.5{]}, Movement (Abstract): 47.8 (2.7) {[}27.2, 15{]},
Spread (Title): 46.6 (2.25) {[}16.2, 12.1{]}, Facility (Abstract): 45
(2.22) {[}19.6, 14.8{]}, Orange County (Keyword): 44.3 (3.19) {[}199,
17.2{]}, Conduct (Abstract): 42.6 (3.7) {[}0.221, -2.57{]}, Patient
(Abstract): 42 (3.61) {[}27.6, 7.23{]}, Perform (Abstract): 41.9 (2.38)
{[}0.342, -2.55{]}, Hospital (Title): 39 (1.95) {[}12.5, 12.5{]}.\\

The ``\&'' indicates nc-ngrams, i.e., terms strongly co-occurrent in the
documents.\\
The engine was able to pick up the central concept of the research
topic, i.e., ``patient transport'' or ``transfer'' through a ``network''
of ``facility''ies that facilitates the ``spread'' of infections, and
even one of the authors of this study (Donker T.) as well as the region
of interest (``Orange County'') of another research group active on the
topic of pathogen spreading over hospital networks. Some terms were
considered highly relevant by the BART models (e.g., ``Worker'' in the
sixth position out of more than 3800 terms considered), although in a
simple linear model, their effect would hardly be significant
(statistic: -1.21 s.e.); these are terms that are only relevant in
conjunction with other terms but not on their own, highlighting the
extra predictive power achieved through the use of advanced, non-linear
machine learning.\\
A more extensive set of terms is presented in Table 1 of Supplemental
Material S2.

\subsection*{Second session}
\addcontentsline{toc}{subsection}{Second session}

The results of the first classification session were used to create a
second, data-driven query to perform a more extensive search to find
records that may have been missed during the first search session. The
resulting query was as follows:\\

\emph{(((Donker T) NOT (bacterium isolate)) OR ((network patient) AND
(resistant staphylococcus aureus) NOT (monte carlo) NOT isolation) OR
(facility AND (network patient) AND regional NOT hospitals NOT increase
NOT (patient transport) NOT (control infection use)) OR ((patient
transport) NOT (Donker T) NOT worker) OR (hospitals AND (network
patient) NOT (patient transport) NOT regional NOT clinical) OR (facility
AND (network patient) NOT hospitals NOT (patient transport) NOT regional
NOT prevention NOT medical) OR ((healthcare facility) NOT (Donker T) NOT
worker NOT positive) OR (hospitals NOT (network patient) NOT medical NOT
environmental NOT outcome NOT global) OR ((network patient) NOT facility
NOT hospitals NOT (patient transport) NOT therapy NOT global)) AND
((antimicrobial resistance) OR (healthcare infection))}\\

The final piece \emph{AND ((antimicrobial resistance) OR (healthcare
infection)} was added manually to define the search domain better since
the algorithm was trained on documents that were all more or less
related to these topics.\\
The generated query also provides a more nuanced understanding of the
engine's internal classification logic, and this is helpful to spot
possible biases in the model.\\

The search was done with the same year filter and procedures used in the
first session.\\

The new search produced 107,294 records (Table 1), of which 48,396
(58.6\%) from the EMBASE, followed by 28,811 (34.9\%) from Pubmed,
17,070 (20.7\%) from SCOPUS, 12,956 (15.7\%) from WOS, and 61 (0.074\%)
from IEEE; compared with the first session, the relative weight of
EMBASE and Pubmed decreased, while the level of content specificity
greatly increased, as it was for SCOPUS. After removal of duplicates,
82,579 unique records were obtained. The newly collected records were
joined with those from the first session and duplicates were removed. We
obtained 98,371 unique records, with just 1,963 shared records between
searches, which equates to 2\% of the total. The percentage of records
shared by two or more sources dropped to 22\%.

Six CR rounds were necessary to complete the second session
classification, with just 6 new positive found after reviewing 568 extra
records. The first CR iteration required the user to review a
substantial number of records (1,273); however, just labelling 275 of
them (the suggested 250 plus 25 already labelled for the framework
hyperparameter tuning) was sufficient to reduce this number to just 190
in the subsequent round. An evaluation of the convergence (Supplemental
Material S2, Fig. 1) showed that, in addition to the dynamics already
observed in session 1 (shrinkage and negative shift), a second mode
appeared in the mixture distribution of the records to be reviewed,
centred in a highly positive zone. The interpretation is that as the
number of negative training records increases, the engine becomes more
and more sceptical and even asks to review some records labelled as
positive in the initial training set generated during Session 1. This
behaviour can rev spot classification errors and inconsistencies.
Considering both sessions, 1,333 records were manually reviewed and 107
(8.03\%) confirmed positive matches were found.

Again, the evaluation of the inclusion rate of the terms showed that the
engine is quite capable of internalising the concepts behind the
research topic. A subsample of the relevant terms used by the model in
the second session is reported in Table 2 of Supplemental Material S2.

\subsection*{Hyperparameter selection}
\addcontentsline{toc}{subsection}{Hyperparameter selection}

As described in the methods, hyperparameters were selected by evaluating
sensibility and efficiency through a grid search on a validation set of
1,200 manually labelled records. The analysis suggested that the
following parameter combination performed best: an initial training set
of 250 categorised records with 10x oversampling of positive matches,
ten models in the ensemble, no bootstrapping and an uncertainty zone
defined by the 98\% predictive interval. This combination of parameters
was associated with a sensitivity of 98.8\% (81 / 82 positive matches
found) and an efficiency of 61.5\% (462 / 1200 records evaluated). The
detailed results of the hyperparameter tuning analysis are reported in
Table 3 of Supplemental Material S2. Fig. 2 in Supplemental Material S2
demonstrates that the positive record oversampling rate, the number of
ensemble models and the size of the initial training set were the
parameters that mainly impacted performance.

\subsection*{Performance evaluation}
\addcontentsline{toc}{subsection}{Performance evaluation}

To evaluate the theoretical performance of the engine, a surrogate
Bayesian logistic regression model was trained on the manually reviewed
labels using only the lower boundary of the record PPDs as predictor
(see the Methods for details). The surrogate model showed the high
predictive power of the scores produced by the classification model
(Bayesian R2: 98.1\% {[}97.4\%, 98.3\%{]} for session 1 and 98.2\%
{[}97.6\%, 98.3\%{]} for session 2).\\

Fig. 2 presents the actual and predicted (from the surrogate model)
cumulative number of positive matches, ordered by the initial simple
ordering query: the median of surrogate models' cumulative predictive
distributions matches the actual number of positive records found quite
well. It is striking how many more records would have required manual
evaluation to find the same number of positive matches without a
classification algorithm, with some positive matches found close to the
end of the heuristically ordered list of records.\\

Table 3 shows various performance indexes for both sessions, both
descriptive (Total records, Reviewed records, Observed positive matches)
and estimated through the surrogate model (Expected efficiency,
Predicted positive matches, Expected sensitivity, \(R^2\)).\\
In session 1 we observe an expected total number of positives of 101
{[}101, 108{]} for an estimated sensitivity of 100\% {[}93.5\%, 100\%{]}
and efficiency of 95.6\% {[}95.3\%, 95.7\%{]}. In session 2 we observed
a drop in expected sensitivity, especially in the lower credibility
boundary (97.3\% {[}72.8\%, 100\%{]}): as the number of records
increases, even a small probability of being a positive match can, in
the worst-case scenario, lead to a relevant number of predicted positive
matches (147 in this case). To ensure no obvious positive matches were
missed, we evaluated 100 non-reviewed records with the highest median
predicted probability and found no additional positive matches.\\

\section*{Discussion}
\addcontentsline{toc}{section}{Discussion}

We propose a new integrated framework to help researchers collect and
screen scientific publications characterised by high performance and
versatility. This framework joins the joining the growing field of
systematic review automation (SRA) and helpers (SRH) tools
\citep{cohen2006reducing, cohen2010evidence, ananiadou2009supporting, o2015using}.
This framework implements standard approaches and uses ad-hoc solutions
to common SRA issues. By freely sharing the tool as an open-source R
package and by following a modular design, we sought to adopt some of
the so-called Vienna Principles advocated by the International
Collaboration for the Automation of Systematic Reviews (ICASR)
\citep{beller2018making}.\\
The framework consists of four main components: 1) an integrated
query-based citation search and management engine, 2) a Bayesian active
machine learning-based citation classifier, and 3) a data-driven search
query generation algorithm.\\

The search engine module used by the framework can automatically collect
citation data from three well-known scientific databases (i.e., Pubmed,
Web of Science, and the Institute of Electrical and Electronics
Engineers) and process manually downloaded results from two more sources
(SCOPUS, EMBASE). In comparison, most commercial or free SRH tools rely
on internal databases (e.g., Mendeley https://www.mendeley.com/)
sometimes focusing only on a particular topic
\citep{visser2010performing} or a single external data source
\citep{thomas2007eppi, poulter2008mscanner, soto2019thalia}.\\
Mixing different databases is essential to obtain a more comprehensive
view of the literature
\citep{bajpai2011search, wilkins2005embase, woods1998medline}: in our
results, 18.7\% of the positive matches were found in only one of the
different data sources, and no positive record was present in all the
sources (data not shown).\\
The framework online search algorithms are efficient enough to manage
tens of thousands of search results, using various solutions to overcome
the limitations of citation databases in terms of traffic and download
quotas. The results are then automatically organised, deduplicated and
arranged by ``simple query ordering'' in a uniform corpus. The
preliminary ordering increases the positivity rate in the initial
training set \citep{wallace2010active}.\\

For the framework's record screening module, we developed an active
machine learning protocol \citep{settles2009active, miwa2014reducing}
based on the best practices from other SRA studies, bringing further
improvements at various levels.\\
The feature extractor module uses modern NLP techniques
\citep{ananiadou2006text, cohen2008getting} to transform free text into
input data for machine learning. We did not include classical n-grams
\citep{schonlau2017text}; rather, we used network analysis to find
non-consecutive, frequently associated terms, a generalisation of
n-grams that relaxes the term adjacency assumption. This approach can
also incorporate term connections across different parts of the records,
e.g., terms having a different relevance when associated with a
particular author. The same technique was used with different parameters
to merge redundant terms, increasing estimation efficiency and reducing
noise.\\
The use of concurrency network-driven text modelling is not new
\citep{rousseau2015graph, violos2016sentiment, rousseau2015text, ohsawa1998keygraph}
and is a valuable tool to extract semantic information that is not
evident in one-word or consecutive n-gram models.\\

The automatic classification algorithm is based on Bayesian Additive
Regression Trees (BART)
\citep{chipman2010bart, kapelner2013bartmachine}. Like other boosted
trees algorithms \citep{hastie2009boosting}, the BART method can explore
complex non-linearities, perform variable selection, manage missing data
while maintaining high predictive power.\\
However, the Bayesian foundation of the method provides further
benefits: lower sensitivity to the choice of hyperparameters, natural
regularisation through priors, and, most importantly, predictive
distributions as output instead of point-wise predictions
\citep{soria2011belm, joo2020being, jospin2020hands}. By selecting
relatively tight prior distributions, we discouraged overly deep trees,
long tree sequences, and extreme predicted probabilities, thus reducing
the risk of overfitting.\\
The algorithm runs multiple replications of the model and averages their
predictive distributions creating an ``ensemble''; this technique has
been shown to improve out-of-sample predictive performance
\citep{zhou2021ensemble, dietterich2000ensemble}, as confirmed during
the hyperparameter evaluation (Supplemental Material S2). Ensembling
reduces the uncertainty in the predictive distribution tails related to
the randomness in the MCMC fit \citep{robert2004monte}, generating a
shift in the probability mass towards the distribution centre and
stabilising it (i.e., reducing variance without impacting bias). On the
other hand, simply imposing more robust uninformative priors against
extreme predictions would have reduced variance but also shifted the
distribution towards a non-decision zone, increasing bias
\citep{hansen2000bayesian}.\\
Since the number of model replications has a significant impact on
computation times, we decided to use ten replicas, the lower value after
which performance stabilised, as resulted from the evaluation of the
hyperparameters (Supplemental Material S2, Fig. 2).\\
We also investigated whether bootstrapping between replications
\citep{breiman1996bagging} would improve performance; however, contrary
to theory \citep{diez2015diversity}, it appeared to be slightly
detrimental in our case (Supplemental Material S2, Fig. 2) compared to
simple ensembling.\\

A low proportion of relevant matches (class imbalance) is typical for
literature reviews
\citep{sampson2011precision, wallace2010semi, o2015using}, and a strong
imbalance between positive and negative records can affect sensitivity
\citep{khoshgoftaar2010comparing, chawla2004special}.\\
To overcome this problem, we oversampled \citep{batista2004study} the
positive records ten times before model fitting. The hyperparameter
analysis showed that the oversampling rate, together with model
ensembling, was the parameter with the most significant impact on
performance.\\
A known risk with positive oversampling is the misclassification of
negative records \citep{ramezankhani2016impact}. However, since all
predicted positives in our approach are reviewed manually, we are always
guaranteed to achieve 100\% specificity/positive predictive value: the
only price for the increased sensitivity due to oversampling is a larger
number of records to be reviewed.\\
An alternative to oversampling would be to apply different weights
and/or costs to the classes \citep{abd2013review, diez2015diversity},
but the BART implementation we used did not have this feature;
furthermore, using simple oversampling allows for a broader
compatibility with different modelling engines
\citep{galar2011review, roshan2020improvement}.\\
Finally, sorting the records by query term frequency (simple query
ordering) produces a much higher rate of relevant records in the initial
training set (17.2\%) compared to the overall data (0.11\%), which
boosts the sensitivity of the model.\\

One of the key innovations we have introduced is the concept of
``uncertainty zone'', the implementation of which is possible thanks to
the Bayesian foundation of the classification model.\\
This construct guides the selection of records to be manually reviewed
and gets dynamically updated and reduced after each CR iteration, as
more uncertain predictions are evaluated (Supplemental Material S2 Fig.
1).\\
The use of a dynamic uncertainty zone overcomes the usual requirement of
dataset-specific hard thresholds in active machine learning and allows
to review multiple items at once between iterations
\citep{laws2008stopping, miwa2014reducing, zhu2010confidence}. The
hyperparameters required by our algorithm are general and
non-task-specific, like the PPD intervals underlying the uncertainty
zone and the maximum number of iterations without positive matches after
which a session is concluded; the evaluation of the classification model
hyperparameters shows that the algorithm is robust against variations in
these parameters, and we expect the default values to perform well on
most datasets.\\
Since researchers are asked to review both records predicted as surely
relevant and those inside the uncertainty zone, this method can be
considered as a unifying synthesis of the ``certainty'' and
``uncertainty'' paradigms of active learning \citep{miwa2014reducing}.\\

We assessed performance as the ability of the screening procedure
(automatic classification plus manual review) to find the largest number
of relevant records while requiring manual reviewing for as few of them
as possible (i.e., sensitivity \(\times\) efficiency).\\
We avoided the classical out-of-sample approaches such as train-test
sampling, out-of-bag bootstrapping or cross-validation
\citep{kohavi1995study, james2013introduction}. Such methods primarily
assume that the rate of positivity is the same on average in every
possible random subset of the data \citep{tashman2000out}; this
uniformity is broken by how the initial training set and the subsequent
reviewed records are selected by the query-based ordering and active
learning algorithm, resulting in a lower positivity rate in the
unlabelled records (Fig. 2). Moreover, a literature corpus is unique per
search query/database combination, and therefore any out-of-sample
performance estimate is not replicable since no new data can be acquired
related to the current corpus.\\
To estimate overall sensitivity, we instead applied simple Bayesian
regression (surrogate model) to the manually reviewed data to abstract
the classification model predictions and generate a maximum entropy
\citep{harremoes2001maximum} estimate of the number of missed positive
matches among the unreviewed records in the whole dataset. This simple
surrogate model fitted the data very well (\(R^2\) consistently above
97\%) using only the lower 98\% PrI boundary of the PPDs as predictor,
indicating predictive consistency in the classification model. The
posterior predictive distribution of the surrogate model could be used
to explore worse case scenarios in terms of sensitivity.\\

Our framework achieves very high sensitivity by screening only a very
small fraction of all records, bringing a meaningful reduction in
workload.\\
Based on the surrogate model, we predicted a predicted median
sensitivity of 100\% {[}93.5\%, 100\%{]} in the first session (screening
4.29\% of records) and of 97.3\% {[}73.8\%, 100\%{]} in the second
(screening 1.34\% of records): efficiency increased significantly in the
second session as only a few new positive matches were found; however,
given the large number of records, uncertainty about sensitivity
increased, as expected.\\
Both results are above the usual performance in this field
\citep{o2015using} and are in line with the average sensitivity of 92\%
estimated after human-only screening \citep{edwards2002identification}.
In one interesting case, the model detected a human-caused
misclassification error, demonstrating its robustness and value as a
second screener, a role already suggested for SRA tools in previous
studies
\citep{frunza2010building, bekhuis2012screening, bekhuis2010towards}.
Although ``simple query ordering'' concentrated most relevant matches in
the first 20-25 thousand records, without the tool support, the
remaining relevant records would have been missed without manually
screening almost the entire dataset.\\

The model required \textasciitilde5-20 minutes per iteration to perform
the predictions in session 1 (17,755 documents) and 20-40 minutes in
session 2 (98,371 documents) on an eight-core, 2.5 GHz, 16 GB RAM, 2014
laptop; including manual record review, one session required 1-3 days of
work, for a total of 1-2 weeks for the whole process (including record
collection). This is a considerable time saving compared to the several
months typically required for the screening phase of systematic reviews
\citep{bannach2019machine, borah2017analysis, allen1999estimating}. To
our knowledge, the amount of data processed (\textasciitilde100,000
records) was larger than what is typical of most SRA studies
\citep{o2015using, olorisade2016critical}, highlighting the scalability
of the tool in real-world scenarios.\\

The last module of our framework is an algorithm for data-driven search
query generation. Generating an efficient and effective search query is
a complex task \citep{lefebvre2011searching, hammerstrom2010searching};
it requires building a combination of positive and negative terms to
maximise the number of relevant search results while minimising the
total number of records to be reviewed. Our solution combines a
sensitivity-driven subquery proposal engine based on concurrent decision
trees \citep{blanco2019machine, moore2018transparent} built on the BART
ensemble PPD, with a human review step and an efficiency-driven query
builder. The aim is to generate a new search query to help find records
missed in the first search session. The generated query did indeed
retrieve a few more relevant records not found in session 1 but at the
cost of significantly increasing the number of documents.\\
An interesting aspect of this feature is that it provides a
human-readable overview of the classification rules learned by the
classification model, showing which combination of terms was
particularly relevant and even spotting authors and geographical
locations associated with the study topic. The generated query,
therefore, served also as a means for machine learning explainability
\citep{bhatt2020machine, burkart2021survey}, useful for understanding
and detecting biases in black-box classification algorithms
\citep{malhi2020explainable}; explainability is often required or even
legally mandatory for high-stake machine learning applications
\citep{bibal2021legal, bibal2020impact}.\\
It is important to note that this process is entirely data-driven. The
algorithm is only aware of the ``world'' defined by the dataset used as
input, which is generated by a specific search query focused on a
particular topic. Therefore, the new query may not be specific enough
when applied to an unbounded search domain and may return an
unmanageable amount of irrelevant results. The solution we found was to
add another component to the query, specifying the general topic
(antimicrobial resistance and healthcare-associated infections) of our
research.\\

As mentioned early, our framework builds on modularity. We have designed
so that each module can become fully independent in future iterations;
it will be possible for users to add custom features such as citation
search and parsing for other scientific databases, alternative text
processing algorithms or machine learning modules. We consider such
interoperability to be extremely relevant: the main strength of our tool
lies in the composition of many independent solutions, such as the idea
of Bayesian active machine learning and the exploit of the derived
uncertainty in defining the records needing human review.\\
Each component could benefit considerably from the recent improvements
in text mining and machine learning.\\
For example, the text processing approach based on the ``boolean
bag-of-words'' paradigm is quite simple and could be improved by more
nuanced text representations. It might be considered whether feature
transformations such as TF-IDF
\citep{baeza1999modern, ananiadou2006text} could be advantageous,
although we hypothesise that tree-based classification algorithms like
BART are robust enough not to require such operations. Instead, it might
be worth exploring the application of word embedding: this technique
transforms terms into semantic vectors derived from the surrounding text
\citep{turian2010word, bollegala2015embedding, minaee2021deep} and could
be used to reduce noise by merging different terms that are semantically
similar or enhance signal by distinguishing identical terms with
different meaning given the context. Another option would be to employ
unsupervised learning models like Latent Dirichlet Analysis and Latent
Semantic Analysis,
\citep{pavlinek2017text, chen2016short, landauer1998introduction} or
graph-of-word techniques \citep{ohsawa1998keygraph, rousseau2015graph}
to extract topics that expand the feature space.\\
Our classification algorithm is applicable with any Bayesian supervised
machine learning method that provides full PPDs; therefore, alternative
classification models, such as Gaussian Processes, known for their
flexibility \citep{jayashree2020evaluation, chen2015gaussian}, could be
evaluated. It would be even more interesting to test advanced learning
algorithms that go beyond the bag-of-words approach and take into
consideration higher-level features in the text such as term context and
sequences, long-distance term relationships, semantic structures, etc.,
\citep{cheng2019document, minaee2021deep, li2020survey, yang2020survey, lai2015recurrent, farkas1995document},
provided that a Bayesian implementation of such algorithms is available
(for example \citet{chen2018approximate}).\\
Finally, a natural improvement would be to provide a graphical user
interface to make the framework easy to use also for less technical
users.

The field of literature review automation is evolving rapidly, and we
anticipate an increasing use of such technologies to address the
accelerating pace of scientific production. We believe it is encouraging
that a wide variety of tools are being made available to let researchers
and policymakers find the approach that best fits their needs.\\
We contribute to this field with an innovative framework that provides
excellent performance and easy integration with existing systematic
review pipelines. The value of this work lies not only in the framework
itself, which we make available as open-source software, but also in the
set of methodologies we developed to solve various SRA issues and which
can also be used to improve existing solutions.\\


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Backmatter begins here                   %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{backmatter}

\section*{Acknowledgements}%% if any
We would like to thank Deborah Lawrie-Blum and Fabian Bürkin
respectively for English language proofreading and mathematical
formalization proof check.

\section*{Abbreviations}%% if any
API: Application Programming Interface; BART: Bayesian Additive
Regression Trees; COVID-19: Coronavirus Disease 2019; CR: Classification
\& Review; DTM: Document-Term Matrix; ICASR: International Collaboration
for the Automation of Systematic Reviews; IEEE: Institute of Electrical
and Electronics Engineers; IS: Inclusion Stability; MCMC: Monte Carlo
Markov Chain; MeSH: Medical Subject Headings; MCMC: Monte Carlo Markov
Chain; NLP: Natural Language Processing; PPD: Expected Value Posterior
Predictive Distribution; RR: Relative Risk; SRA: Systematic Review
Automation; SRH: Systematic Review Helpers; TF-IDF: Term Frequency -
Inverse Document Frequency; WOS: Web Of Science; WSoR: Work Saved over
Random.

\vspace{2em}
\section*{Declarations}
\vspace{1em}

\section*{Funding}%% if any
This project was developed under the Joint Programming Initiative on
Antimicrobial Resistance (JPIAMR) through the 7th call, project number
01KI1831 funded by BMBF and administrated by DLR Project Management
Agency.

\section*{Availability of data and materials}%% if any
The code and the instructions necessary to reproduce the results are
available at
https://github.com/AD-Papers-Material/BART\_SystReviewClassifier. An
updated, ready-to-use version of the framework is available at
https://github.com/bakaburg1/BaySREn. All relevant data necessary to
replicate the results is available at:
https://doi.org/10.5281/zenodo.6323360.

\section*{Ethics approval and consent to participate}%% if any
Not applicable.

\section*{Competing interests}
The authors declare no competing interests.

\section*{Consent for publication}%% if any
Not applicable.

\section*{Authors' contributions}
Conceptualization: AD, TD; Data curation: AD; Formal analysis \&
Methodology: AD; Project administration: AD, TD; Software development:
AD; Supervision: TD, HG; Writing--original draft: AD, TD;
Writing--review \& editing: AD, TD, HG. All authors have read and
approved the manuscript.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                  The Bibliography                       %%
%%                                                         %%
%%  Bmc_mathpys.bst  will be used to                       %%
%%  create a .BBL file for submission.                     %%
%%  After submission of the .TEX file,                     %%
%%  you will be prompted to submit your .BBL file.         %%
%%                                                         %%
%%                                                         %%
%%  Note that the displayed Bibliography will not          %%
%%  necessarily be rendered by Latex exactly as specified  %%
%%  in the online Instructions for Authors.                %%
%%                                                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% if your bibliography is in bibtex format, use those commands:
\bibliographystyle{bmc-mathphys} % Style BST file (bmc-mathphys, vancouver, spbasic).

\phantom{\citet{d2022open}, \citet{zenododata}}

\bibliography{references.bib}      % Bibliography file (usually '*.bib' )
% for author-year bibliography (bmc-mathphys or spbasic)
% a) write to bib file (bmc-mathphys only)
% @settings{label, options="nameyear"}
% b) uncomment next line
%\nocite{label}

% or include bibliography directly:
% \begin{thebibliography}
% \bibitem{b1}
% \end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Figures                       %%
%%                               %%
%% NB: this is for captions and  %%
%% Titles. All graphics must be  %%
%% submitted separately and NOT  %%
%% included in the Tex document  %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% Do not use \listoffigures as most will included as separate files

\section*{Figures}
		\begin{figure}[h!]
	 \caption*{Figure 1. Framework's visual representation.}
	\end{figure}
		\begin{figure}[h!]
	 \caption*{Figure 2. Observed cumulative number of positive matches
(red dots) sorted by simple query ordering. The {[}trunc. 90\% PrI{]} of
the cumulative positive matches estimated by the Bayesian logistic model
is shown as a shaded area delimited by the 95\% quantile of the PrI and
by the observed number of positive matches (light blue lines). A darker
blue line represents the median of the PrI.}
	\end{figure}
	%   \begin{figure}[h!]
%   \caption{Sample figure title}
% \end{figure}
%
% \begin{figure}[h!]
%   \caption{Sample figure title}
% \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Tables                        %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Use of \listoftables is discouraged.
%%
\section*{Tables}
	\begin{table}[H]
\caption{\label{tab:Table 1}\textbf{Table 1}. Distribution of retrieved records by source and session. For each source, we reported the number of records, percentage over the session total (after removing duplicates), and the number of records specific for a source as absolute value and as percentage over the source total. All session shows records after joining and deduplication of the Session 1 and Session 2 data set.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{llrlrl}
\toprule
Session & Source & Records & \% over total & Source specific records & \% over source total\\
\midrule
\cellcolor{gray!6}{Session1} & \cellcolor{gray!6}{Total} & \cellcolor{gray!6}{17,755} & \cellcolor{gray!6}{} & \cellcolor{gray!6}{} & \cellcolor{gray!6}{}\\
 & Embase & 12,719 & 71.6\% & 6,683 & 52.5\%\\
\cellcolor{gray!6}{} & \cellcolor{gray!6}{Pubmed} & \cellcolor{gray!6}{9,546} & \cellcolor{gray!6}{53.8\%} & \cellcolor{gray!6}{3,457} & \cellcolor{gray!6}{36.2\%}\\
 & Scopus & 3,175 & 17.9\% & 298 & 9.39\%\\
\cellcolor{gray!6}{} & \cellcolor{gray!6}{WOS} & \cellcolor{gray!6}{2,100} & \cellcolor{gray!6}{11.8\%} & \cellcolor{gray!6}{473} & \cellcolor{gray!6}{22.5\%}\\
\addlinespace
 & IEEE & 60 & 0.34\% & 29 & 48.3\%\\
\cellcolor{gray!6}{Session2} & \cellcolor{gray!6}{Total} & \cellcolor{gray!6}{82,579} & \cellcolor{gray!6}{} & \cellcolor{gray!6}{} & \cellcolor{gray!6}{}\\
 & Embase & 48,396 & 58.6\% & 40,826 & 84.4\%\\
\cellcolor{gray!6}{} & \cellcolor{gray!6}{Pubmed} & \cellcolor{gray!6}{28,811} & \cellcolor{gray!6}{34.9\%} & \cellcolor{gray!6}{18,021} & \cellcolor{gray!6}{62.5\%}\\
 & Scopus & 17,070 & 20.7\% & 4,908 & 28.8\%\\
\addlinespace
\cellcolor{gray!6}{} & \cellcolor{gray!6}{WOS} & \cellcolor{gray!6}{12,956} & \cellcolor{gray!6}{15.7\%} & \cellcolor{gray!6}{2,817} & \cellcolor{gray!6}{21.7\%}\\
 & IEEE & 61 & 0.074\% & 22 & 36.1\%\\
\cellcolor{gray!6}{All Sessions} & \cellcolor{gray!6}{Total} & \cellcolor{gray!6}{98,371} & \cellcolor{gray!6}{} & \cellcolor{gray!6}{} & \cellcolor{gray!6}{}\\
 & Embase & 59,604 & 60.6\% & 46,942 & 78.8\%\\
\cellcolor{gray!6}{} & \cellcolor{gray!6}{Pubmed} & \cellcolor{gray!6}{37,278} & \cellcolor{gray!6}{37.9\%} & \cellcolor{gray!6}{21,371} & \cellcolor{gray!6}{57.3\%}\\
\addlinespace
 & Scopus & 19,353 & 19.7\% & 5,181 & 26.8\%\\
\cellcolor{gray!6}{} & \cellcolor{gray!6}{WOS} & \cellcolor{gray!6}{14,367} & \cellcolor{gray!6}{14.6\%} & \cellcolor{gray!6}{3,175} & \cellcolor{gray!6}{22.1\%}\\
 & IEEE & 108 & 0.11\% & 48 & 44.4\%\\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[H]
\caption{\label{tab:Table 2}\textbf{Table 2}. Results of the automatic classification and manual review rounds. The cumulative numbers of positives and negative records and their sum (Total labelled) and percentage over total are shown for each iteration. Also, the number of changes after review and their description is reported."Unlab." indicates unlabelled records marked for review. For each iteration, the number of features used by the engine is also reported. The first row reports the results of the initial manual labelling of records, which served as input for the automatic classification in Iteration 1. In Session 2, the engine uses the labels at the end of Session 1 to classify the newly added records.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{llrrlrrrrrr}
\toprule
Session & Iteration & Positives & Negatives & Total labelled (\%) & Unlab. -> y & Unlab. -> n & Unlab. -> * & n -> y & Changes & N. features\\
\midrule
\cellcolor{gray!6}{Session1 (n = 17755)} & \cellcolor{gray!6}{Initial labelling} & \cellcolor{gray!6}{43} & \cellcolor{gray!6}{207} & \cellcolor{gray!6}{250 (1.41\%)} & \cellcolor{gray!6}{43} & \cellcolor{gray!6}{207} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{250} & \cellcolor{gray!6}{2,289}\\
 & 1 & 93 & 529 & 622 (3.5\%) & 50 & 322 & 0 & 0 & 372 & 2,289\\
\cellcolor{gray!6}{} & \cellcolor{gray!6}{2} & \cellcolor{gray!6}{100} & \cellcolor{gray!6}{614} & \cellcolor{gray!6}{714 (4.02\%)} & \cellcolor{gray!6}{6} & \cellcolor{gray!6}{86} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{1} & \cellcolor{gray!6}{93} & \cellcolor{gray!6}{3,750}\\
 & 3 & 101 & 625 & 726 (4.09\%) & 1 & 11 & 0 & 0 & 12 & 3,834\\
\cellcolor{gray!6}{} & \cellcolor{gray!6}{4} & \cellcolor{gray!6}{101} & \cellcolor{gray!6}{648} & \cellcolor{gray!6}{749 (4.22\%)} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{23} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{23} & \cellcolor{gray!6}{3,856}\\
\addlinespace
 & 5 & 101 & 651 & 752 (4.24\%) & 0 & 3 & 0 & 0 & 3 & 3,856\\
\cellcolor{gray!6}{} & \cellcolor{gray!6}{6} & \cellcolor{gray!6}{101} & \cellcolor{gray!6}{660} & \cellcolor{gray!6}{761 (4.29\%)} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{9} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{9} & \cellcolor{gray!6}{3,856}\\
 & 7 & 101 & 665 & 766 (4.31\%) & 0 & 5 & 0 & 0 & 5 & 3,856\\
\cellcolor{gray!6}{Session2 (n = 98371)} & \cellcolor{gray!6}{1} & \cellcolor{gray!6}{106} & \cellcolor{gray!6}{934} & \cellcolor{gray!6}{1040 (1.06\%)} & \cellcolor{gray!6}{5} & \cellcolor{gray!6}{270} & \cellcolor{gray!6}{998} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{1,273} & \cellcolor{gray!6}{4,729}\\
 & 2 & 107 & 1,123 & 1230 (1.25\%) & 1 & 189 & 0 & 0 & 190 & 4,729\\
\addlinespace
\cellcolor{gray!6}{} & \cellcolor{gray!6}{3} & \cellcolor{gray!6}{107} & \cellcolor{gray!6}{1,176} & \cellcolor{gray!6}{1283 (1.3\%)} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{53} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{53} & \cellcolor{gray!6}{4,733}\\
 & 4 & 107 & 1,200 & 1307 (1.33\%) & 0 & 24 & 0 & 0 & 24 & 4,729\\
\cellcolor{gray!6}{} & \cellcolor{gray!6}{5} & \cellcolor{gray!6}{107} & \cellcolor{gray!6}{1,209} & \cellcolor{gray!6}{1316 (1.34\%)} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{9} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{0} & \cellcolor{gray!6}{9} & \cellcolor{gray!6}{4,729}\\
 & 6 & 107 & 1,226 & 1333 (1.36\%) & 0 & 17 & 0 & 0 & 17 & 4,729\\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}[H]
\caption{\label{tab:Table 3}\textbf{Table 3}. Estimated performance summary. The table reports for each session, the number of reviewed records and the percentage over the total. Also, the posterior expected number of positive records, sensitivity and efficiency (as WSoR) are reported, with their 90\% PrI truncated to the observed realisation in the dataset [trunc. PrI] (see. methods). Finally, the logistic model's median Bayesian $R^2$ [90\% CrI] is reported. PrI: Predictive Intervals; CrI: Credibility Intervals.}
\centering
\begin{tabular}[t]{lll}
\toprule
Indicator & Session 1 & Session 2\\
\midrule
\cellcolor{gray!6}{Total records} & \cellcolor{gray!6}{17,755} & \cellcolor{gray!6}{98,371}\\
Reviewed records (\% over total records) & 766 (4.31\%) & 1,333 (1.36\%)\\
\cellcolor{gray!6}{Expected efficiency (over random) [trunc. 90\% PrI]} & \cellcolor{gray!6}{95.6\% [95.3\%, 95.7\%]} & \cellcolor{gray!6}{98.6\% [98.1\%, 98.6\%]}\\
Observed positive matches (\% over total records) & 101 (0.57\%) & 107 (0.11\%)\\
\cellcolor{gray!6}{Predicted positive matches [trunc. 90\% PrI]} & \cellcolor{gray!6}{101 [101, 108]} & \cellcolor{gray!6}{110 [107, 147]}\\
\addlinespace
Expected sensitivity [trunc. 90\% PrI] & 100\% [93.5\%, 100\%] & 97.3\% [72.8\%, 100\%]\\
\cellcolor{gray!6}{Simple Model $R^2$ [90\% CrI]} & \cellcolor{gray!6}{98.1\% [97.4\%, 98.3\%]} & \cellcolor{gray!6}{98.2\% [97.6\%, 98.3\%]}\\
\bottomrule
\end{tabular}
\end{table}
	% \begin{table}[h!]
% \caption{Sample table title. This is where the description of the table should go}
%   \begin{tabular}{cccc}
%     \hline
%     & B1  &B2   & B3\\ \hline
%     A1 & 0.1 & 0.2 & 0.3\\
%     A2 & ... & ..  & .\\
%     A3 & ..  & .   & .\\ \hline
%   \end{tabular}
% \end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Additional Files              %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Additional Files}
  \subsection*{S1. Framework description and usage.}
  Instruction on how to use the framework and reproduce the results in
the manuscript.
  \subsection*{S2. Additional outputs.}
  Additional analysis outputs described in the manuscript.
  \subsection*{S3. Online search instructions.}
  Instructions about how to prepare the framework to interact with the
online scientific databases to collect records.
  % \subsection*{Additional file 1 --- Sample additional file title}
  %   Additional file descriptions text (including details of how to
  %   view the file, if it is in a non-standard format or the file extension).  This might
  %   refer to a multi-page table or a figure.
  %
  % \subsection*{Additional file 2 --- Sample additional file title}
  %   Additional file descriptions text.

\end{backmatter}
\end{document}
