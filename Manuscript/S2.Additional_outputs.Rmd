---
title: "S2. Additional figures and table"
date: "`r lubridate::today()`"
output:
  pdf_document:
    toc: yes
    keep_tex: true
    includes:
      in_header: header.tex
  github_document:
    toc: yes
---

\newpage

```{r setup, include=FALSE, message=FALSE, warning=FALSE, error = FALSE}

wd <- ifelse(basename(getwd()) == "Manuscript", dirname(getwd()), getwd())

knitr::opts_knit$set(root.dir = wd)

options(knitr.kable.NA = "")

knitr::opts_chunk$set(
  echo = FALSE,
  error = FALSE,
  message = FALSE,
  warning = FALSE,
  dpi = 400,
  cache = TRUE,
  cache.rebuild = FALSE
)
```

```{r load framework, cache.rebuild=TRUE}
source(file.path("R", "Setup.R"))

# The following are already the default, but better be explicit
options(baysren.probs = c(.05, .5, .95))
options(baysren.sessions_folder = "Sessions")
```


# Posterior predictive distributions

Figure 1 show the mixture of the PPD of the probability of a positive match, grouped by label (only records manually labelled or reviewed are considered).
The posterior samples for each record were extracted and joined into a global distribution;
on this distribution, density was computed on the logit scale and then logistic transformed for displaying on a $[0 - 1]$ scale.
The purple ridges show the distribution of the still unlabelled records.\
For each iteration, the thresholds which define the "uncertainty zone", i.e, the lower and the upper range of 98% PrI for the positive and negative records respectively, are shown.
Records whose 98% PrI intersects the uncertainty zone requires manual review.\
Notice how the positive and negative record densities tend to increasingly overlap at each iteration;
meanwhile the distribution of the records to be reviewed shrinks and shifts towards the negative side, as positive records get found and labeled.\
In Session 2, as the number of negative records reviewed increased, also already positively labelled records re-entered the uncertainty zone;
this is due to the baseline positivity rate decreasing as the number of negatives in the training data increased, forcing to review dubious records that may have been mislabelled.

```{r posterior_distributions, fig.width=10, fig.height=10, fig.cap="**Figure 1**. Mixture predictive distribution of the probability of a positive match, grouped by labelling status."}

s1_plot <- plot_predictive_densities("Session1")

s2_plot <- plot_predictive_densities("Session2")

s1_plot +
  guides(color = "none", fill = "none") +
  ggtitle("Session 1") +
  s2_plot +
  ggtitle("Session 2") +
  theme(
    axis.title.y = element_blank()
  ) &
  theme(
    legend.position = "bottom"
  ) &
  plot_layout(guides = "collect")
```

# List of terms relevant for prediction

```{r var imp}

n_terms <- 50

var_importance_s1 <- extract_var_imp("Session1", num_vars = n_terms, score_filter = 0) %>%
  format_var_imp(as_data_frame = TRUE)

var_importance_s2 <- extract_var_imp("Session1", num_vars = n_terms, score_filter = 0) %>%
  format_var_imp(as_data_frame = TRUE)
```

In table 1 and 2 are listed the `r n_terms` more relevant terms used by the BART algorithm to discriminate between positive and negative records, for Session 1 and 2 (see Methods).
Term importance ("Inclusion rate" in the tables) is defined as the ensemble average inclusion rate in posterior trees over 10,000 total term inclusions, while the Inclusion Stability (IS) is the ratio of the average inclusion rate over its standard deviation among the ensemble models.
The symbol "|" in the terms indicate redundant terms while "&" indicate nc-grams.
The component in which the term was used is reported in the leftmost column.\
For each term, we added its linear association with a positive label estimated through a Poisson regression, reporting it as Relative Risk (RR) and its statistical significance (Statistic) measured as number of standard errors, s.e., of the terms.
A strong BART score with a Statistic close to zero identify terms whose effect is highly non-linear (e.g., they are relevant only in the presence of other terms).

`r print_table(var_importance_s1, caption = '**Table 1**. Term importance at the end of Session 1.')`


`r print_table(var_importance_s2, caption = '**Table 2**. Term importance at the end of Session 2.')`

# Hyperparameters grid search

Table 3 shows the best hyperparameter set and their performance for each cluster while figure 2 displays the conditional impact on performance of each hyperparameter.

The most influential parameters were the number of models in the ensemble, the oversampling rate and the initial training set size. The use of bootstrap resampling seemed detrimental.

```{r hyperparameters, fig.width=17, fig.height=10, fig.cap="**Figure 2**. Score clusters and impact of single hyperparameters on engine performance. The performance is measured as Sensitivity x Efficiency. Each cluster is color coded. Mods: number of models in the ensemble; Quant: PrI quantiles for the uncertainty zone; Resamp: bootstrap resampling between models; Init: size of initial training set; Multi: oversampling rate of positive records"}

grid_out <- analyse_grid_search()

grid_out_params <- grid_out$best_by_rule %>%
  mutate(Score = str_remove(Score, " .+")) %>%
  rename(
    `Cluster (mean score)` = Cluster,
    `Num. iterations` = Iter,
    `Positive matches` = Pos_labels,
    `Reviewed records` = Tot_labeled,
    `Score (Sens. x Eff.)` = Score,
    `Num. ensemble models` = Mods,
    `Uncertainty interval` = Quant,
    `Resampling` = Resamp,
    `Num. initial training records` = Init,
    `Positives oversampling multiplier` = Mult
  )

print(grid_out$plot & labs(y = "Sens. x. Eff."))
```


`r print_table(grid_out_params, caption = '**Table 3**. Hyperparameter clusters and best cluster subsets. For each cluster, the defining rules and mean Sens. x Eff. is shown, followed by the per-cluster best set results.')`
