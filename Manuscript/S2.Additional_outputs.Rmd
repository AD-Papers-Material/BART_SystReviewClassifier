---
title: "S2. Additional figures and table"
date: "`r lubridate::today()`"
output:
  # pdf_document:
  #   toc: yes
  #   includes:
  #     in_header: header.tex
  github_document:
    toc: yes
---

\newpage

```{r setup, include=FALSE, message=FALSE, warning=FALSE, error = FALSE}

wd <- ifelse(basename(getwd()) == 'Manuscript', dirname(getwd()), getwd())

knitr::opts_knit$set(root.dir = wd)

options(knitr.kable.NA = '')

knitr::opts_chunk$set(
	echo = FALSE,
	error = FALSE,
	message = FALSE,
	warning = FALSE,
	dpi = 400,
	cache = TRUE,
	cache.rebuild = FALSE
)

```

```{r load framework, cache.rebuild=TRUE}
source(file.path('R', 'Setup.R'))

# The following are already the default, but better be explicit
options(baysren.probs = c(.05, .5, .95))
options(baysren.sessions_folder = 'Sessions')
```


# Posterior predictive distributions

Figure 1 show the mixture of the predictive distribution of the probability of a positive match, grouped by labelling status. The posterior samples for each record were extracted and joined in a global distribution; on this distribution, density was computed on the logit scale and then logistic transformed for display on a $[0 - 1]$ scale. The light violet ridges show the distribution of the still unlabelled records.\
For each iteration, the thresholds which define the lower and the upper range of 98% predictive interval (PrI) respectively for the positive and negative records are shown. The zone included between these two boundaries defines the uncertainty zone; records whose 98% PrI intersects this zone requires manual review.\
Notice how the positive and negative densities start overlapping starting with the second iteration of Session 2. Meanwhile the distribution of the records to be reviewed shrink and shift towards the negative side, as positive records get found and labeled.

```{r posterior distributions, fig.width=10, fig.height=10, fig.cap="Figure 1. Mixture predictive distribution of the probability of a positive match, grouped by labelling status."}

s1_plot <- plot_predictive_densities('Session1')

s2_plot <- plot_predictive_densities('Session2')

s1_plot +
    guides(color = 'none', fill = 'none') +
    ggtitle('Session 1') +
    s2_plot +
    ggtitle('Session 2') +
    theme(
        axis.title.y = element_blank()
        ) &
    theme(
        legend.position = 'bottom'
    ) &
    plot_layout(guides = 'collect')
```

# List of terms relevant for prediction

```{r var imp}

n_terms <- 50

var_importance_s1 <- extract_var_imp('Session1', num_vars = n_terms, score_filter = 0) %>%
	format_var_imp(as_data_frame = TRUE)

var_importance_s2 <- extract_var_imp('Session1', num_vars = n_terms, score_filter = 0) %>%
	format_var_imp(as_data_frame = TRUE)
```

In table 1 and 2 are listed the `r n_terms` more relevant terms used by the BART algorithm to discriminate between positive and negative records, for Session 1 and 2 (see Methods).
Term importance ("Inclusion rate" in the tables) is defined as the ensemble average inclusion rate in posterior trees over 10,000 total term inclusions, while the Inclusion Stability (IS) is the ratio of the average inclusion rate over its standard deviation among the ensemble models.
The symbol "|" in the terms indicate redundant terms while "&" indicate nc-grams.
The component in which the term was used is reported in the leftmost column.\
For each term, we added its linear association with a positive label estimated through a Poisson regression, reporting it as Relative Risk (RR) and its statistical significance (Statistic) measured as number of standard errors, s.e., of the terms.
A strong BART score with a Statistic close to zero identify terms whose effect is highly non-linear (e.g., they are relevant only in the presence of other terms).

`r print_table(var_importance_s1, caption = 'Table 1. Term importance at the end of Session 1.')`


`r print_table(var_importance_s2, caption = 'Table 2. Term importance at the end of Session 2.')`

# Hyperparameters grid search

To select the engine hyperparameter set which would maximize sensitivity and efficiency, we set up a comprehensive grid search by running the algorithm on a fully labeled dataset of 1200 records. Using a partition tree algorithm we grouped the searches in a number of clusters with similar performance and selected the best parameter set in the best cluster.\
Table 3 shows the best parameter set and their performance for each cluster while figure 2 displays the conditional impact on performance of each parameter.

```{r hyperparameters, fig.width=17, fig.height=10, fig.cap="Figure 2. Performance clusters and impact of single hyperparameters on engine performance. The performance is measured as Sensitivity x Efficiency. Each cluster is color coded. Mods: num. of models in the ensemble; Quant: uncertainty zone interval to trigger manual review; Resamp: bootstrap resampling; Init: num. of initial training records; Multi: oversampling multiplier of positive matches."}

grid_out <- analyse_grid_search() 

grid_out_params <- grid_out$best_by_rule %>% 
	mutate(Score = str_remove(Score, ' .+')) %>% 
	rename(
		`Cluster (mean score)` = Cluster,
		`Num. iterations` = Iter,
		`Positive matches` = Pos_labels,
		`Reviewed records` = Tot_labeled,
		`Score (Sens. x Eff.)` = Score,
		`Num. ensemble models` = Mods,
		`Uncertainty interval` = Quant,
		`Resampling` = Resamp,
		`Num. initial training records` = Init,
		`Positives oversampling multiplier` = Mult
	)

print(grid_out$plot & labs(y = 'Sens. x. Eff.'))

```


`r print_table(grid_out_params, caption = 'Table 3. Hyperparameter clusters and best cluster subsets. For each cluster, the defining rules and mean Sens. x Eff. is shown, followed by the per-cluster best set results.')`
