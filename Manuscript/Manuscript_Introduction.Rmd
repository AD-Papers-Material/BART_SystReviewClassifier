---
title: "Introduction"
date: "`r lubridate::today()`"
bibliography: references.bib
csl: apa.csl
output:
  pdf_document:
    includes:
      in_header: header.tex
  github_document: default
---

## Introduction

Scientific production has experienced continuous exponential growth in the last decades [@larsen2010rate; @bornmann2015growth]. This is especially true for biomedical research, a trend further increased by the COVID19 pandemic, thanks to decreased journals' processing time and the diffusion of preprint databases' usage [@aviv2021publication; @horbach2020pandemic; @hoy2020rise]. Consequently, it gets harder for researchers and practitioners to stay up-to-date on the latest findings in their field. Secondary research is of paramount relevance in this scenario, providing valuable summaries of the state of the art, but is getting ever more demanding in terms of time and human resources [@allen1999estimating; @borah2017analysis; @cohen2010evidence; @bastian2010seventy].\
The article collection and screening phases of a systematic review are particularly problematic tasks [systematic review tutorial]. First, relevant published research needs to be collected from scientific databases through appropriately built search queries. The construction of search queries is a particularly delicate task [@lefebvre2011searching; @hammerstrom2010searching], requiring both domain and some knowledge of the databases' query languages; the goal is to produce a set of results containing all relevant articles (high sensitivity) while keeping the total number low (high specificity), focusing on the first aspect at the cost of the second [@hammerstrom2010searching]. If an integrated search tool is not used, manual work is required to download, store and organize the publication data. Searching manually may be complicated by limits in the number of records that can be downloaded at once [] and the necessity to harmonize different formats and resolve duplication. The citation screening phase consists of classifying the collected publications as relevant or not for the topic of interest [@bannach2019machine]. It is usually the more resource-demanding task of a systematic review: even with appropriately built search queries, the returned results easily range in the tens of thousands of which just a small fraction is actually relevant [@lefebvre2011searching]. It was estimated that labelling 10 000 publications may take as much as 40 weeks of work, while the average clinical systematic review takes 63 weeks to be completed [@bannach2019machine; @borah2017analysis; @allen1999estimating]. A consequence is that often systematic reviews are already outdated once they are published [@beller2013systematic].\
The field of Data Science applied to evidence synthesis and acquisition has greatly maturated in the last years [@marshall2015systematic; @beller2018making; @tsafnat2014systematic]. Through the application of natural language processing (NLP), it is possible to transform free text into quantitative features, with various levels of abstraction and generalization [@ananiadou2006text; @cohen2008getting]; with machine learning (ML), such text-derived data can be used to map and reproduce human judgment and automatize the citation screening [@ikonomakis2005text]. One of the most successful approaches to text screening is active ML: researchers review the predictions made by the ML algorithm, focusing on the most uncertain predictions ("uncertainty" based active ML)  or on those giving a document a higher probability of being relevant ("certainty" based active ML) [@miwa2014reducing].\
The automatization of systematic reviews has been ripe with improvements in the last years, and it is possible to foresee that such techniques will become a standard approach [@beller2018making]. See @ananiadou2009supporting; @o2015using; @tsafnat2013automation; @jonnalagadda2015automating for in-depth evaluations of research in this field. Many of these studies have generated a set of user-friendly commercial and free-to-use tools [see @marshall2015systematic, table 1].
Despite the significant advances, each method has various shortcomings, both in usability and efficiency [@o2015using].
We tried to address some of these limitations by creating an integrated framework that tries to simplify systematic reviews in innovative ways: our framework provides solutions for automatic citation collection and management from multiple online sources and a Bayesian, active machine learning-based tool to support humans in the screening task, exponentially reducing the number of records to review. In particular, we exploit the characteristics of Bayesian analysis to overcome the shortcoming of both "certainty" and "uncertainty" based active learning and the difficulties in defining stopping rules [@miwa2014reducing].
Finally, we propose an experimental method to semi-automatically generate efficient search queries given an already labelled set of publications.\
We apply this solution in the production of a systematic review evaluating the mathematical modelling of patient referral networks among hospitals and their impact on the diffusion of healthcare-associated pathogenic microorganisms with a focus on antimicrobial-resistant strains. The systematic review protocol is published in [].

