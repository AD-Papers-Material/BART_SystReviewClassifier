---
title: "Discussion"
date: "`r lubridate::today()`"
bibliography: references.bib
csl: apa.csl
output:
  pdf_document:
    includes:
      in_header: header.tex
  github_document: default
---

# Discussion

We propose a new integrated framework to help researchers collect and screen scientific publications characterised by high performance and versatility, joining the growing field of systematic review automation (SRA) and helpers (SRH) tools [@cohen2006reducing; @cohen2010evidence; @ananiadou2009supporting; @o2015using].
This framework implements standard approaches and uses ad-hoc solutions to deal with common SRA issues.
By freely sharing the tool as an open-source R package and by following a modular design, we tried to adopt some of the so-called Vienna Principles advocated by the International Collaboration for the Automation of Systematic Reviews (ICASR) [@beller2018making].\
The framework consists of four main components: 1) an integrated query-based citation search and management engine, 2) a Bayesian active machine learning-based citation classifier, and 3) a data-driven search query generation algorithm.\

The framework's search engine module is capable of automatically collecting citation data from three well-known scientific databases (i.e., Pubmed, Web of Science, and the database of the Institute of Electrical and Electronics Engineers) as well as process manually downloaded results from both the two more (SCOPUS, EMBASE) databases.
In comparison, most SRH tools, commercial or free to use, rely either on internal databases (e.g., Mendeley https://www.mendeley.com/) sometimes focusing just on a particular topic [@visser2010performing] or on a single external data source [@thomas2007eppi; @poulter2008mscanner; @soto2019thalia].\
Mixing different databases is fundamental to have a more comprehensive view of the literature [@bajpai2011search; @wilkins2005embase; @woods1998medline]:
in our results, 18.7% of the positive matches found only in one of the different data sources, and no positive record was present in all of them (data not shown).\
The online search algorithms are efficient enough to manage tens of thousands of search results, using various expedients to overcome the limitations of online citation databases in terms of traffic and download quotas.
The results are then automatically organized, deduplicated and arranged by "simple query ordering" in a uniform corpus.
The preliminary ordering allows to increase the positivity rate in the initial training set [@wallace2010active].\

For the framework's record classification module, we employed an active machine learning approach [@settles2009active, @miwa2014reducing] based on the best practices from other SRA studies but also bringing further improvements at various levels.\
The feature extractor module uses modern NLP techniques [@ananiadou2006text; @cohen2008getting] to transform text into input data for machine learning.
We did not include classical n-grams [@schonlau2017text], but we used network analysis to find non-consecutive frequently associated terms, a generalisation of n-grams relaxing the term adjacency assumption.
This approach allows also to find term connections across fields, for example a term having a different relevancy if associated with a specific author.
The same technique but with different parameters was applied to merge redundant terms to make model estimation more efficient and reduce noise.\
The use of concurrency network-driven modelling of text is not new [@rousseau2015graph; @violos2016sentiment; @rousseau2015text; @ohsawa1998keygraph] and is a valuable tool to extract semantic information not evident in one-word or consecutive n-gram models.\

The automatic classification algorithm is based on Bayesian Additive Regression Trees (BART) [@chipman2010bart; @kapelner2013bartmachine].
As with other boosted trees algorithms [@hastie2009boosting], BART can explore complex non-linearities, perform variable selection, manage missing data while sporting high performance in predictive power.\
However, its Bayesian foundation provides further benefits:
less sensitivity on hyperparameter choices, natural regularisation through priors, and, especially, predictive distributions as output in place of point-wise predictions [@soria2011belm; @joo2020being; @jospin2020hands].
By selecting relatively tight prior distributions, we discouraged excessively deep trees, long sequences of trees, or extreme predicted probabilities, to decrease the risk of overfitting.\
The algorithm runs multiple replications of the model and averages their predictive distributions creating an "ensemble";
this technique has been shown to improve out-of-sample predictive performance [@zhou2021ensemble; @dietterich2000ensemble], as we were able to confirm during the hyperparameter evaluation (Supplemental Material S2).
Ensembling reduces the uncertainty in the predictive distribution tails related to the randomness in the MCMC fit [@robert2004monte], generating a shift of probability mass towards the distribution centre and stabilising it (i.e., decreasing variance without impacting bias).
On the other hand, just imposing robust uninformative priors against extreme predictions would have decreased variance but also shifted the distribution towards a non-decision zone, increasing bias [@hansen2000bayesian].\
Since the number of model replications significantly impacts computation times, we decided to use ten replicas, the lower value after which performance stabilised during the hyperparameter evaluation.\
We also investigated whether bootstrapping between replications [@breiman1996bagging] would improve performance, but, contrary to theory [@diez2015diversity], it appeared to be slightly detrimental in our case (Supplemental Material S2) compared to simple ensembling.\

A low rate of relevant matches (class imbalance) is typical in literature reviews [@sampson2011precision; @wallace2010semi; @o2015using], and such strong imbalance between positive and negative records can affect sensitivity [@khoshgoftaar2010comparing; @chawla2004special].\
To overcome the problem, we oversampled [@batista2004study] the positive records ten times before model fitting.
Our hyperparameter analysis showed that together with model ensembling, the oversampling rate was the parameter with the highest impact on performance.\
A known risk with positive oversampling is the misclassification of negative records [@ramezankhani2016impact].
However, since all predicted positives get manually reviewed in our approach, we are always ensured to achieve 100% specificity/positive predictive value: the only price for the increased sensitivity due to oversampling is a larger number of records to review.\
An alternative to oversampling would be applying different weights and/or cost to the classes [@abd2013review; @diez2015diversity], but the BART implementation we used did not have this feature;
also, using simple oversampling permits broader compatibility with different modelling engines [@galar2011review; @roshan2020improvement].\
Finally, ordering the records by query term frequency (simple query ordering) generates a far higher rate of relevant records in the initial training set (17.2%) compared to the overall data (0.11%), and this boosts the sensitivity of the model.\

One of the central innovations we introduced is the concept of "uncertainty zone", whose implementation is possible thanks to the Bayesian foundation of the classification model.
This construct guides the selection of the records to review, dynamically updating and shrinking after every CR iteration, as more uncertain predictions are evaluated (Supplemental Material S2 Fig. 1).\
This approach overcomes the usual requirement of dataset-specific hard thresholds in active machine learning, and also allows to review multiple items at once between iterations [@laws2008stopping; @miwa2014reducing; @zhu2010confidence].
The parameters our algorithm needs are instead general and non task-specific, like the PPD intervals based on which the uncertainty zone is built, and the maximum number of iterations with no positive matches after which a session is concluded;
the hyperparameter evaluation shows that the algorithm is robust against variations in these parameters and we expect the default values to perform well on most datasets.\
Since researchers are asked to review both records with a positive predicted label and those inside the uncertainty zone, this method can be considered as a unifying synthesis of the "certainty" and "uncertainty" paradigms of active learning [@miwa2014reducing].\

We evaluated performance as the capability of the screening procedure (automatic classification plus manual review) to find the largest number of relevant records while reviewing as few of them as possible (i.e., sensitivity $\times$ efficiency).\
We avoided the classic out-of-sample approaches like train-test sampling, out-of-bag bootstrapping or cross-validation [@kohavi1995study; @james2013introduction].
Such methods primarily assume that the rate of positivity is equal on average in every random subset of the data [@tashman2000out];
this uniformity is broken by how the initial training set and the subsequent reviewed records are selected by the query-based ordering and the active learning algorithm, determining a lower positivity rate in the unlabelled records (Fig. 2).
Also, a literature corpus is unique per search query/database combination, and therefore any out-of-sample performance estimate is not replicable since no new data can be acquired related to the current corpus.\
Instead, to estimate overall sensitivity, we employed simple Bayesian regression (surrogate model) on the manually reviewed data to abstract the classification model predictions and achieve a maximum entropy [@harremoes2001maximum] estimate of the number of missed positive matches among the unreviewed records in the whole dataset.
This simple surrogate model fitted the data very well ($R^2$ consistently above 97%) using just the lower 98% PrI bound of the PPDs as predictor, indicating predictive consistency in the classification model.
The surrogate model posterior predictive distribution could be exploited to explore worse case scenarios in terms of sensitivity.\

Our framework achieved very high sensitivity by screening a markedly small fraction of all records, bringing a sensible reduction in workload.\
Based on the surrogate model, we predicted a predicted median sensitivity of 100% [93.5%, 100%] in the first session (screening 4.29% of records) and of 97.3% [73.8%, 100%] in the second (screening 1.34% of records):
efficiency increased significantly in the second session since only a few new positive matches were found, but given the large number of records, uncertainty regarding sensitivity also expectedly increased.\
Both results are above the usual performance in the field [@o2015using] and in line with the 92% average sensitivity estimated after human-only screening [@edwards2002identification].
In one interesting case, the model spotted a human-made misclassification error, demonstrating its robustness and value as a second screener, a role already suggested for SRA tools by previous studies [@frunza2010building; @bekhuis2012screening; @bekhuis2010towards].
Finally, albeit the simple query ordering already concentrated most of the relevant matches in the first 20-25 thousand records, without the tool support some relevant records would have required almost the complete data set to be manually checked to be found.\

The model took ~5-20 minutes per iteration to perform predictions in session 1 (17,755 documents) and 20-40 minutes in session 2 (98,371 documents) on an eight-core, 2.5 GHz, 16 GB RAM laptop from 2014;
including manual record review, one session required 1-3 days of work, for a total of 1-2 weeks for the whole process (including record collection).
That is a considerable saving of time compared to the multiple months usually required for the screening phase of systematic reviews [@bannach2019machine; @borah2017analysis; @allen1999estimating].
To our knowledge, the amount of data processed (~100.000 records) were larger than what is typical in most SRA studies [@o2015using; @olorisade2016critical], emphasising the reliability of the tool in real-world scenarios.\

The last module of our framework is a data-driven query generation algorithm.
Creating an efficient and efficacious search query is a complex task [@lefebvre2011searching; @hammerstrom2010searching];
it requires building a combination of positive and negative terms to maximise the number of relevant search results while minimising the total number of records to review.
Our solution joins a sensitivity-driven subquery proposal engine based on concurrent decision trees [@blanco2019machine; @moore2018transparent] built on the BART ensemble PPD, with a human review step and an efficiency-driven query builder.
The aim is to generate a second query that helps find records missed during the first session search.
The generated query allowed indeed to retrieve few more positive matches not found in session 1, but at the cost of a significant increase in the number of documents.\
One interesting aspect of this functionality is that it provides a human-readable overview of the classification rules learned by the classification model, showing which combination of terms was particularly relevant and even spotting authors and geographical locations associated with the study topic.
The generated query, therefore, acted as a tool for machine learning explainability [@bhatt2020machine; @burkart2021survey], a feature useful to understand and spot bias in black-box classification algorithms [@malhi2020explainable];
explainability is often required or even legally mandatory for high-stake machine learning applications [@bibal2021legal; @bibal2020impact].\
It is important to note that this process is entirely data-driven.
The algorithm is only aware of the "world" defined by the data set used as input, which is generated by a specific search query focused on a particular topic.
Therefore, the new query may not be specific enough once applied to an unbounded search domain, returning an unmanageable amount of unrelated results.
The solution we found was to add another component to the query, specifying the general topic (antimicrobial resistance and healthcare-associated infections) of our research.\

As reported, our framework builds on modularity.
We designed it to easily implement complete independence of the main modules in future iterations, making it possible for users to add custom features like citation search and parsing for other scientific databases, alternative text processing algorithms or machine learning modules.
We deem such interoperability extremely relevant because the main strength of our tool is the composition of many solutions and the general idea of Bayesian active machine learning and the exploit of the derived uncertainty in predictions.
However, each of its components could benefit considerably from the recent improvements in text mining.\
For example, our text processing approach is quite simple, based on the boolean bag-of-words paradigm, and indeed could be improved by more nuanced text representations.
It could be evaluated if feature transformations like TF-IDF [@baeza1999modern; @ananiadou2006text] would be advantageous, even if we hypothesise that tree-based classification algorithms like BART are robust enough not to need such operations.
Word embedding could be instead worth exploring:
this technique transforms terms in semantic vectors derived from the surrounding text [@turian2010word; @bollegala2015embedding, @minaee2021deep] and could be used to eliminate semantically redundant terms or differentiate identical terms with different meanings given the context.
Another option would be to employ unsupervised learning models like Latent Dirichlet Analysis, Latent Semantic Analysis [@pavlinek2017text; @chen2016short; @landauer1998introduction] or graph-of-word techniques [@ohsawa1998keygraph; @rousseau2015graph] to extract topics to enrich the feature space.\
Our classification algorithm can be implemented with any Bayesian supervised machine learning method that provides full PPDs;
therefore alternative classification models could be evaluated, like Gaussian Processes which are known for their flexibility [@jayashree2020evaluation; @chen2015gaussian].
Even more interesting would be to test advanced learning algorithms that surpass the bag-of-words approach, taking into consideration higher-level features in the text like term context and sequences, long-distance term relationships, semantic structures, etc., [@cheng2019document; @minaee2021deep; @li2020survey; @yang2020survey; @lai2015recurrent; @farkas1995document], given that a Bayesian implementation of such algorithms is available (for example @chen2018approximate).\
Finally, a natural improvement would be to provide a graphical user interface to make the framework easy to use also for less technical users.

The field of literature review automation is maturing rapidly, and we expect an increasing use of such technologies to manage the ever-faster rate of scientific production.
We believe it is appreciable that a multiplicity of tools are being made available to let researchers and policymakers find the instrument that better fits their needs.\
We contribute to this field with an innovative framework that provide excellent performance and easy integration with existing systematic review pipelines.
The value of this work lies not only in the framework itself, which we provide as open-source software, but in the set of methodologies we developed to solve various SRA issues and that can be used to improve already existing solutions.\
