---
title: "Discussion"
date: "`r lubridate::today()`"
bibliography: references.bib
csl: apa.csl
output:
  pdf_document:
    includes:
      in_header: header.tex
  github_document: default
---

## Discussion

We propose a new integrated framework to help researcher collect and screen research publications characterized by high performance and versatility, joining the growing field of systematic review automatization (SRA) and helpers (SRH) tools.\
The International Collaboration for the Automation of Systematic Reviews (ICASR) proposes a number of features tools for automatic research synthesis should possess, defining the so called Vienna Principles [@beller2018making].We tried to adopt some of the concepts advocated by the ICAS; for example we are sharing the tool as an open source R package, and we tried to follow the paradigms of modular programming [@mccracken2003modular], building separated components communicating through intermediate outputs stored in common formats (Comma Separated Values/Microsoft Excelâ„¢), which make it possible for users to extend the framework or integrate it with other tools in their pipeline.\
Our framework has two main components, an integrated query-based citation search and management engine and an active machine learning record screener\

Our search engine is capable of automatically collecting citation data from three well known scientific databases, that is, Pubmed, Web of Science and the database of the Institute of Electrical and Electronics Engineers, implementing also institutional access rights; in addition, it can process manually downloaded results from all mentioned databases plus records from the SCOPUS and EMBASE databases. Once the citations are downloaded, they are translated in a uniform format and deduplicated. The online search algorithm is efficient enough to manage tens of thousands of search results, using various expedients to overcome the online databases limitations in terms of traffic and download quota. Most SRH tools we are aware of, commercial or free to use, rely either on internal databases (e.g., Mendeley https://www.mendeley.com/), often focused on a particular topic [@visser2010performing] or relying on only one external data source [@thomas2007eppi; @poulter2008mscanner; @soto2019thalia]. To our knowledge, our tool is the first capable to fully automatize searching on three different well known scientific databases and to import raw data from five of them. Mixing different database is fundamental to have a more complete view of the literature on a topic [@bajpai2011search; @wilkins2005embase; @woods1998medline]: in our results, 18.7% of the positive matches were unique for one of the different data sources and no positive record was present among all of them (data not shown).\

Modern NLP techniques were used to transform textual data into actionable numeric inputs [@ananiadou2006text; @cohen2008getting]. One particular improvement was to use network analysis to identify associated terms, which we labelled as non-consecutive ngrams, which confer particular meaning when present together into a text while not necessary being close to each other. A similar network approach was used to merge redundant terms, in order to make model estimation more efficient and reduce possible noise. The use of network inspired modeling of text is not new [@rousseau2015graph; @violos2016sentiment; @rousseau2015text] and is a valuable tool to extract semantic information not evident in one-word of consecutive ngram models.

The citation screening functionality of the framework implements active machine learning based document classification, adopting best practices and bringing further improvements at various levels. At the base of our classification algorithm we use BART [@chipman2010bart; @kapelner2013bartmachine], a Bayesian machine learning technique characterized by exceptional predictive performance and speed even in large dataset. This model builds consecutive decision trees associating the presence/absence of set of terms to the probability of a positive match, each tree trying to decrease the prediction error of the precedent one [@hastie2009boosting]. The Bayesian approach explore distributions of trees which are justified by the data, while using a set of prior distributions to avoid overfitting by discouraging excessively deep trees, long sequences of trees, or extreme predicted probabilities. To further improve generalizability we ran multiple replications of the model and then averaged their record predictive distributions; this technique is called ensembling and it has been shown to improve out-of-sample predictive performance []. In a Bayesian setting, this effect comes from a reduction of the probability mass in the tails of the predictive distributions while not shifting their locations (i.e., it decreases variance without impacting bias) [bias/variance tradeoff, better if in Bayesian analysis and/or focusing to ensemble]. Instead, imposing stronger general priors against extreme predictions would decreased variance but also shifted the distribution mass towards a non-decision zone, therefore increasing bias. In our data it can be seen how using more then one model replica greatly improved sensitivity and efficiency, with a performance stabilization after 10 replicas (Suppl. Mat. S2); since the number of model replicas greatly impacted computation times we decided to use ten replicas since it was also associated with the higher sensitivity in the same performance cluster. The variability between models derives only from the randomness in the MCMC fitting phase []; an alternative was to impose additional variability by bootstrapping [] but this approach actually decreased performance (Suppl. Mat. S2), probably due to the low proportion of positive matches in the data and the loss of information with bootstrapping in unbalanced classification tasks [].\
The strong imbalance between relevant and non-relevant records [@sampson2011precision] can affect sensitivity []. To overcome the problem, we oversampled the positive records ten times before model fitting. Our hyperparameter analysis showed that together with model ensembling, the oversampling rate was the parameter with the highest impact on performance. An alternative approach would be record weighting to change error costs during estimation [] but the BART implementation we used did not allow observation weighting and using simple oversampling allows to use different modeling engines. Also, the simple query ordering of the record during the creation of the initial training set allowed to have a far higher prevalence of relevant records (17.2%) compared to the overall one (0.11%) and this boosts the overall sensitivity of the model. A known risk with oversampling of the target class is the missclassification of negative records as positive [], but since in our approach all predicted positive are manually reviewed we are ensured to achieve always 100% specificity/positive predictive value.\
One of the innovation in our approach is the overcoming of simple stopping rules in SRA classification tasks []. Static supervised learning models requires to manually label a certain amount of records (i.e., the training set) which are then used to train the algorithm and classify the rest (the test set) [], but this method requires to decide the size of the training set in advance. Active learning tries to alleviate the problem by splitting the classification task in an iterative process in which prediction and review phases alternate, but even in this case, one has to define a stopping rule regarding how many records to screen manually in every iteration []. By exploiting the Bayesian nature of BART we produce a distribution of probabilities and not a single value for each record; by joining these distribution in the positive and negative record groups it's possible to identify a "uncertainty zone" so that all record whose predictive distribution crosses this zone needs manual verification. Therefore the researcher will have a defined number or records to check at every iteration which dynamically decreases as uncertain predictions are reviewed and the uncertainty shrinks (Suppl. Mat. S2 Fig. 1). In this model, researcher do not need to choose a priori the number of records to review but just the size of the uncertainty zone (by setting the PPD quantiles used to built it) and after how many iterations with no new positive matches to stop; both the parameters are more theoretical and general than a simple stopping rule and nevertheless the hyperparameter tuning did not indicate a big impact of these parameters. Since our approach requires researchers to review both unlabelled records with a positive predicted label and those with a PPD inside the uncertainty zone, it can be considered as a unifying syinthesis of the "certainty" and "uncertainty" paradigms of active learning [@miwa2014reducing].\

To evaluate performance we avoided the classic random out-of-sample approaches like train-test sampling, out-of-bag bootstrapping or cross-validation [@kohavi1995study] because we deemed them not realistic in this setting. It is assumed that the corpus to be screened is not a random sample, but the entire population of citation for a given query; a random validation would be useful to estimate the performance of this method on a new dataset derived by the same data-generating process, but this is a totally theoretical goal with no usefulness since all the existing data related was alread acquired. Instead a more relevant question is to estimate the long run sensitivity of our prediction method in finding all relevant records in the whole data set. Since the records are ordered, again a random approach would not work since it assumes that the prevalence of positive records is equal on average in every possible sample [@tashman2000out], while instead it drops as more records are considered (Fig. 2). We instead used a Bayesian model to estimate the likelihood of a positive match given the prediction scores of the BART model. This approach showed to fit the data very well (high $R^2$) and the Bayesian predictive distribution can be exploited to draw a worse case scenario given the data.\
In our study our approach reached 98.8% sensitivity during the hyperparameter optimization (deterministic, small sample ) and on the whole data set we predicted a theoretical sensitivity of 100% [93.5%, 100%] in the first session and of 97.3% [73.8%, 100%] in the second, a lower value as is expected by the probabilistic accumulation of random events; both results are above the usual results in the field [@o2015using] and in line with the 92% average sensitivity estimated after human only screening [@edwards2002identification]. In one interesting case, the model spotted a missclassification during initial labeling, demonstrating its robustness and its value also as a second screener, as already suggested by previous studies [@frunza2010building; @bekhuis2012screening; @bekhuis2010towards].\
Also we showed that such sensitivity was achieved screening just 1.34% of all downloaded records, a sensible reduction in workload; finally, albeit the simple query based ordering did concentrate most of the relevant matches in the first 20-25 thousands records, some relevant records would have required almost the full data set to be manually checked to be found.\
The model takes ~5-20 minutes per iteration to perform predictions in session 1 (17755 documents) and 20-40 minutes in session 2 (98371 documents) on a 8 core, 2.5 GhZ, 16 GB RAM laptop from 2014 and that equate to 1-3 days per session to manually review and perform predictions, for a total of 1-2 weeks for the whole process, a huge saving of time compared to the months usually required for the screening phase of systematic reviews [@bannach2019machine; @borah2017analysis; @allen1999estimating]. To our knowledge, our data sets are larger than what common in most SRA studies [@o2015using; @olorisade2016critical], emphasizing the reliability of the tool in real world scenarios.\

The last component of our framework is a data-driven query generation algorithm. The creation of an efficient and efficacious search query is a complex task [@lefebvre2011searching; @hammerstrom2010searching] since it requires building a combination of positive and negative terms to maximize the number of relevant search results while minimizing the total number of results. We propose a solution based on concurrent decision trees [@blanco2019machine; @moore2018transparent] built on the posterior predicted probabilities; the technique tries to extract high sensitivity subqueries from the labeled data, enrich them with negative terms to increase specificity [@abdelmgeid2008using] and then let researcher evaluate them and pick the most meaningful ones. The framework will then join them and remove redundancies. The aim is to generate a second query that complement the first human made one and help find possible missing records not found during the first session.\
The generated query allowed to retrieve few more positive matches not found in session 1 at the cost of a large increase in the amount of documents. One interesting aspect of this functionality is that it provides a human-readable overview of the classification rules internalized by the classification model, showing which combination of terms was particularly relevant and even spotting authors and locations associated to the topic of study. The generated query therefore acted as a tool for machine learning explainability [@bhatt2020machine; @burkart2021survey], a feature useful to spot bias in black box classification algorithms [@malhi2020explainable] and that is increasingly legally required for high-stack machine learning applications [@bibal2021legal; @bibal2020impact].\^
It is important to note that this process is entirely data-driven. The algorithm is only aware of the "world" defined by the data set, itself generated by a specific search query focused on a particular topic. Therefore, the new query may not be specific once applied to an unbounded search domain, returning an unmanageable amount of unrelated results. The solution we found was to add another component specifying the general topic (antimicrobial resistance and healthcare associated infections) of our research.\

As reported, our framwork builds on modularity. We designed it in order to easily implement complete independency of the main modules in future iterations; especially, we plan to allow users create their custom made components in addition or in place of the original ones, further expanding its functionalities. In our view it would be possible to easily add custom citation search and parsing functions for other scientific databases, alternative text features building algorithms and alternative machine learning modules.\
We deem such interoperability extremely relevant, because the main strength of our tool is the composition of many solutions and the general approach related to Bayesian active machine learning, but each of its components could benefit greatly from the recent improvements in text mining.\
For example, our text feature generation approach is based on the boolean bag-of-words paradigm, and surely it could be improved by more nuanced text representations: it could be evaluated if feature transformations like Tf-Idf [@baeza1999modern; @ananiadou2006text], would be advantageous, even if we hypothesize that tree based classification algorithms like BART are robust enough to not need such operation. Word embedding could be considered, which transform terms in semantic vectors based on the surrounding text [@turian2010word; @bollegala2015embedding, @minaee2021deep], and could be used to eliminate semantically redundant terms or differentiate identical terms with different meanings given the context; another option would be to use unsupervised learning models like Latent Dirichlet Analysis, Latent Semantic Analysis, etc., [@pavlinek2017text; @chen2016short; @landauer1998introduction] to extract topics to enrich the feature space.\
Our classification algorithm can be implemented with any Bayesian supervised machine learning method which produces full PPDs; therefore alternative models could be evaluated, like Gaussian Processes which are known for their flexibility [@jayashree2020evaluation; @chen2015gaussian]. Even more interesting would be to test  advanced learning algorithms that surpass the bag-of-words approach, taking into consideration higher level features in the text like term context and sequences, long distance term relationships, semantic structures, etc., [@cheng2019document; @minaee2021deep; @li2020survey; @yang2020survey; @lai2015recurrent; @farkas1995document], given that a Bayesian implementation of such algorithms is available (for example @chen2018approximate).\

